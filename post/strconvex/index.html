<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Amartya Sanyal">

  
  
  
    
  
  <meta name="description" content="I am going to , inspired by the course on optimization that I am doing this semester, talk a bit about strong convexity and strong smoothness and our very popular gradient descent works on them. So, before going right into the details let&#39;s have a quick chat about convexity in general and we do have a few ways of going about it.
I will go about with talking about two definitions of convex functions, the first one being general more than the second.">

  
  <link rel="alternate" hreflang="en-us" href="https://amartya18x.github.io/post/strconvex/">

  


  
  
  
  <meta name="theme-color" content="#3f51b5">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://amartya18x.github.io/post/strconvex/">

  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@AmartyaSanyal">
  <meta property="twitter:creator" content="@AmartyaSanyal">
  
  <meta property="og:site_name" content="Amartya Sanyal">
  <meta property="og:url" content="https://amartya18x.github.io/post/strconvex/">
  <meta property="og:title" content="Strong Convexity and Strong Smoothness | Amartya Sanyal">
  <meta property="og:description" content="I am going to , inspired by the course on optimization that I am doing this semester, talk a bit about strong convexity and strong smoothness and our very popular gradient descent works on them. So, before going right into the details let&#39;s have a quick chat about convexity in general and we do have a few ways of going about it.
I will go about with talking about two definitions of convex functions, the first one being general more than the second."><meta property="og:image" content="https://amartya18x.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="https://amartya18x.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2016-09-15T23:49:40&#43;05:30">
    
    <meta property="article:modified_time" content="2020-01-23T11:07:53&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://amartya18x.github.io/post/strconvex/"
  },
  "headline": "Strong Convexity and Strong Smoothness",
  
  "datePublished": "2016-09-15T23:49:40+05:30",
  "dateModified": "2020-01-23T11:07:53Z",
  
  "author": {
    "@type": "Person",
    "name": "Amartya Sanyal"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Amartya Sanyal",
    "logo": {
      "@type": "ImageObject",
      "url": "https://amartya18x.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "I am going to , inspired by the course on optimization that I am doing this semester, talk a bit about strong convexity and strong smoothness and our very popular gradient descent works on them. So, before going right into the details let's have a quick chat about convexity in general and we do have a few ways of going about it.\nI will go about with talking about two definitions of convex functions, the first one being general more than the second."
}
</script>

  

  


  


  





  <title>Strong Convexity and Strong Smoothness | Amartya Sanyal</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Amartya Sanyal</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Amartya Sanyal</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Strong Convexity and Strong Smoothness</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Jan 23, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    8 min read
  </span>
  

  
  
  
  <span class="middot-divider"></span>
  <a href="/post/strconvex/#disqus_thread"></a>
  

  
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>I am going to , inspired by the course on optimization that I am doing this semester, talk a bit about strong convexity and strong smoothness and our very popular <strong>gradient descent</strong> works on them. So, before going right into the details let's have a quick chat about convexity in general and we do have a few ways of going about it.</p>
<p>I will go about with talking about two definitions of convex functions, the first one being general more than the second. This is not to say that there aren't more general or equivalent definitions, which there are, but that I like these better and this is probably what you will use more in your life if you have to use one at all.</p>
<h2 id="convex-function">Convex Function</h2>
<p>$$\frac{f(y)+f(x)}{2} \ge f(\frac{x+y}{2}) $$
Note that this does not require the function to be differentiable. But, with differetiable functions, one can actually get an easier and more productive definition.</p>
<h3 id="convex-differetiable-functions">Convex differetiable functions</h3>
<p>$$ f(y) \ge f(x) + \langle \nabla f(x), y - x \rangle $$</p>
<h3 id="subgradients">Subgradients</h3>
<p>The interesting thing about this is that this can tolerate functions, which are not differntiable in a finite number of points. We need to define <strong>subgradients</strong> for that though.</p>
<p>$$ g(x) = \{ g(x) | \langle g(x), x_0 - x\rangle \forall y \} $$</p>
<p>And hence let's do the intuitive thing of replacing  $\nabla f(x)$ with $g(x)$. So, now we have</p>
<p>$$ f(y) \ge f(x) + \langle  g(x), y - x \rangle $$</p>
<h2 id="strongly-convex-function">Strongly Convex function</h2>
<p>Now that we know of convex functions mathematically, it is intuitively a function such that if we draw a tangent plane(line in case of one variable), the function <strong>lies above the tangent plane</strong> at all points. Mathematically, that would be</p>
<p>$$ f(y) \ge f(x) + \langle g(x), y - x \rangle + \frac{\alpha}{2} \| y - x\|^2 $$ and <strong>$\alpha$</strong> is known as the strong convexity parameter. One can also, for doubly differntiable functions, say $ \nabla^2 f(x) \succeq \alpha I  $, where $\nabla^2 f(x)$ is the hessian matrix and $ I $ is the identity matrix.</p>
<h2 id="strongly-smooth-function">Strongly Smooth function</h2>
<p>A strongly smooth function is just the opposite of the strongly convex function i.e. a function which <strong>lies below the tangent plane</strong> at all points. Mathematically, that would be</p>
<p>$$ f(y) \le f(x) + \langle g(x), y - x \rangle + \frac{\beta}{2} \| y - x\|^2 $$ and <strong>$\beta$</strong> is known as the strong smoothness parameter. One can also, for doubly differntiable functions, say $ \beta I \succeq \nabla^2 f(x)  $.</p>
<h1 id="gradient-descent-for-strongly-convex-functions">Gradient descent for strongly convex functions</h1>
<p>We know for a strongly convex function $f(x)$, $$ f(y) \ge f(x) + \langle g(x), y - x \rangle + \frac{\alpha}{2} \| y - x\|^2 $$ A very nice property of these functions is that, we can actually bound $\|f( x^{*}) - f(x)\|$ where $(x^{*})$ is the optimal point , with the norm of the gradient ($\|\nabla f(x)\|$).
Let,
$$
z = \underset{y}{argmin }\hspace{5pt}  \{ f(x) + \langle g(x), y - x \rangle + \frac{\alpha}{2} \| y - x\|^2 \}
$$</p>
<p>The definition of strong convexity is applicable for all y.</p>
<p>\begin{align*}
z &amp;= x - \frac{\nabla f(x)}{\alpha} \\<br>
f(y) &amp;\ge f(x) + \langle g(x), y - x \rangle + \frac{\alpha}{2} \| y - x\|^2  \\<br>
&amp;\ge f(x) + \langle g(x), \frac{\nabla f(x)}{\alpha}  \rangle + \frac{\alpha}{2} \| \frac{\nabla f(x)}{\alpha}  \|^2 \\<br>
&amp;\ge f(x) - \frac{1}{2\alpha} | \nabla f(x)|^2 \\<br>
\therefore f(x^{*}) &amp;\ge f(x) - \frac{1}{2\alpha} \| \nabla f(x)\|^2
\end{align*}</p>
<p>This interesting bound also gives us a good convergence criterion. Set $f(x) - f(x^{*}) = \epsilon$ i.e. the desired error. We only need to do the gradient descent until the gradient has reached $2\alpha \epsilon$ from the previous inequality.
$$
2\alpha \epsilon \le \frac{1}{2\alpha} \| \nabla f(x)\|^2
$$</p>
<p>Like the function value above, we can also get a bound on the distance of the current point from the optimal point in terms of gradient.</p>
<p>\begin{align*}
f(x^{*})  &amp;\ge f(x) + \langle g(x), x^{*} - x \rangle + \frac{\alpha}{2} \| x^{*} - x\|^2  \\<br>
&amp;\ge f(x) - \underbrace{( \{ \| g(x)\|\| x^{*} - x \| - \frac{\alpha}{2} \| x^{*} - x\|^2 \}) }_a &amp;&amp; \text{(By Cauchy's inequality)}\\<br>
\end{align*}
As we know that $f(x^{*})$ is optimal $ f(x^{*}) \le f(x)$, a \ge 0 must be true.</p>
<p>$$ \| g(x)\|\| x^{*} - x \| \ge \frac{\alpha}{2} \| x^{*} - x\|^2  $$
Or,
$$\frac{2}{\alpha} \| g(x)\| \ge   \| x^{*} - x\|  $$</p>
<h2 id="analyzing-the-gradient-descent">Analyzing the gradient descent</h2>
<p>Define,
$$
\Phi_t = f(x^t) - f(x^{*})
$$</p>
<ul>
<li>the lyapunov function. Decrease in the value of the lyapunov function means that the function is nearing its optima. Let's also define another distance.
$$
D_t = \| x^{*} - x_t \|_2
$$
\begin{align*}
\label{eq:phi}
f(x^{*})  &amp;\ge f(x_t) + \langle \nabla f(x_t), x^{*} - x_t \rangle + \frac{\alpha}{2} \| x^{*} - x_t\|^2  \\<br>
f(x^{*}) -  f(x_t)  &amp;\ge \hspace{2pt} \langle\nabla f(x_t), x^{*} - x_t \rangle + \frac{\alpha}{2} \| x^{*} - x_t\|^2 \\<br>
\end{align*}</li>
</ul>
<p>Hence, we have the following inequality with the lyapunov function.
\begin{equation}
\Phi_t \le -\langle g(x_t), x^{*} - x_t \rangle - \frac{\alpha}{2} \| x^{*} - x_t\|^2
\end{equation}
Lets work with $D_t$ and then try to relate it with $\Phi_t$.
\begin{align*}
D^2_{t+1} &amp;= \|x^{t+1} - x^{*} \|_2^2 \\<br>
&amp;= \| x^t - \eta_t g_t -   x^{*}\|_2^2 \\<br>
&amp; = \|x^t - x^{*} \|_2^2 + \eta_t^2 \|g_t(x_t)\|^2  - 2 \eta_t   \langle x^t - x^{*}, g_t\rangle \\<br>
D^2_{t+1} - D^2_{t} &amp;= \eta_t^2 \|g_t(x_t)\|^2  - 2 \eta_t   \langle x^t - x^{*}, g_t\rangle \\<br>
\langle x^t - x^{*}, g_t\rangle &amp;= \frac{D^2_t - D^2_{t+1}}{2\eta_t} + \frac{\eta_t \|g_t\|^2}{2}
\end{align*}
Let's plugin in this onto the previous definition of the lyapunov function.
\begin{align*}
\Phi_t &amp;\le -\langle g(x_t), x^{*} - x_t \rangle - \frac{\alpha}{2} \| x^{*} - x_t\|^2 \\<br>
&amp;\le \frac{D^2_t - D^2_{t+1}}{2\eta_t} + \frac{\eta_t \|g_t\|^2}{2} - \frac{\alpha}{2} \| x^{*} - x_t\|^2 \\<br>
&amp;\le  \frac{D^2_t - D^2_{t+1}}{2\eta_t} + \frac{\eta_t \|g_t\|^2}{2} - \frac{\alpha}{2} D^2_t
\end{align*}
It is difficult to show that $\Phi_t$ is going to zero. It is considerably easier to use the sum for that. We will see a trick with jensen's inequality.
\begin{align*}
\sum_{t=0}^T \Phi_t &amp;\le \sum_{t=0}^T (\frac{D^2_t - D^2_{t+1}}{2\eta_t} + \frac{\eta_t \|g_t\|^2}{2} - \frac{\alpha}{2} D^2_t) \\<br>
&amp;\le  \sum_{t=0}^T (\frac{D^2_t}{2\eta_t} + \frac{\eta_t \|g_t\|^2}{2} - \frac{\alpha}{2} D^2_t) -  \sum_{t=1} \frac{D^2_{t}}{2\eta_{t-1}} &amp;&amp; \text{(A bit of change of variable)} \\<br>
&amp;\le D_0^2(\frac{1}{2\eta_0} - \frac{\alpha}{2}) +  \sum_{t=1}^T D^2_t(\frac{1}{2\eta_t} - \frac{\alpha}{2} + \frac{1}{2\eta_{t-1}}) + \sum_{t=0}^T\frac{\eta_t G^2}{2} &amp;&amp;\text{(Assuming bounded gradients again)}
\end{align*}</p>
<p>Now, the first term is a constant and the third term is a sum of constants weighed by a parameter that we are fixing. So, the major problem is with the second term. Why note set it to zero ? We can do that by setting <code>$\eta\_t = \frac{1}{\alpha t}$</code>
\begin{align*}
\sum_{t=0}^T \Phi_t &amp;\le D_0^2(\underbrace{\frac{1}{2\eta_0} - \frac{\alpha}{2}}_{\le 0}) +  \sum_{t=1}^T D^2_t( \underbrace{\frac{1}{2\eta_t} - \frac{\alpha}{2} + \frac{1}{2\eta_{t-1}}}_0) + \sum_{t=0}^T\frac{ G^2}{2\alpha t} &amp;&amp;\text{(Assuming bounded gradients again)} \\<br>
\frac{1}{T}\sum_{t=0}^T {\Phi_t} &amp;\le \frac{G^2 log(T)}{2T} \\<br>
\sum_{t=0}^T  \frac{f(x^t) - f(x^{*})}{T} &amp;\le \frac{G^2 log(T)}{2T}  \\<br>
\end{align*}
Applying Jensen's inequality because $\Phi_t$ is convex
$$f(\frac{\sum_{t=0}^T x^t}{T}) \le \sum_{t=0}^T  \frac{f(x^t) }{T} - f(x^{*}) \le \frac{G^2 log(T)}{2T} $$
To remove the $log(T)$, try doing a weighed sum of the $\Phi_t$ with $t\Phi_(t)$ and then don't forget to divide by $\sum_{t=1}^{T}t$. It will work out smooth</p>
<h1 id="gradient-descent-for-strongly-smooth-functions">Gradient descent for strongly smooth functions</h1>
<p>We will work with similar <strong>Lyapunov function</strong> <code>$\Phi\_t$</code> and <code>$D\_t$</code></p>
<p>As $f(x)$ is strong smooth,
$$ f(x^{t+1}) \le f(x^t) + \langle \nabla f(x^t), x^{t+1} - x^t \rangle + \frac{\beta}{2} \| x^{t+1} - x^t \|^2 $$
and as $f(x)$ is convex
$$ f(x^{*}) \ge f(x) + \langle \nabla f(x), x^{*} - x \rangle  $$
By rearranging terms we get,
$$  f(x) \le f(x^{*})  + \langle \nabla f(x), x - x^{*}  \rangle  $$
Plugging in this inequality into the strong smoothness inequality and replacing the $\Phi_{t+1}$ and $D_t$ in the correct places gives us the following</p>
<p>\begin{align*}
\Phi_{t+1} &amp;\le   \langle \nabla f(x), x^t - x^{*}\rangle + \langle \nabla f(x^t), x^{t+1} - x^t \rangle + \frac{\beta}{2} \| x^{t+1} - x^t \|^2 \\<br>
&amp;\le  \langle \nabla f(x), x^t - x^{*} +  x^{t+1} - x^t \rangle + \frac{\beta}{2} \| x^{t+1} - x^t \|^2 \\<br>
&amp;\le  \langle \nabla f(x),  x^{t+1} - x^{*} \rangle + \frac{\beta}{2} \| x^{t+1} - x^t \|^2 \\<br>
\end{align*}</p>
<p>Now, we know the following <code>$x^{t+1} = x^t - \eta\_t \nabla f(x^t)$</code> which gives us <code>$\nabla f(x^t) = \frac{x^t - x^{t+1}}{\eta\_t} $</code></p>
<p>\begin{align*}
\Phi_{t+1} &amp;\le  \langle \nabla f(x),  x^{t+1} - x^{*} \rangle + \frac{\beta}{2} \| x^{t+1} - x^t \|^2 \\<br>
&amp;\le  \frac{1}{\eta_t}\langle x^{t} - x^{t+1},  x^{t+1} - x^{*} \rangle + \frac{\beta}{2} \| x^{t+1} - x^t \|^2 \\<br>
\end{align*}
Now, we need to evaluate the term <code>$\langle x^{t} - x^{t+1},  x^{t+1} - x^{*} \rangle $</code>
\begin{align*}
\|x^t - x^{*}\|^2 &amp;= \|x^t - x^{t+1} + x^{t+1} - x^{*}\| \\<br>
&amp;=\|x^t - x^{t+1}\|^2 + \|x^{t+1} - x^{*}\| + 2\langle x^{t} - x^{t+1},  x^{t+1} - x^{*} \rangle \\<br>
\langle x^{t} - x^{t+1},  x^{t+1} - x^{*} \rangle &amp;= \frac{1}{2}( D_{t}^2 - D_{t+1}^2 -  \|x^{t+1} - x^{*}\| )
\end{align*}
Plugging this back into the inequality we have above, we get the following</p>
<p>\begin{align*}
\Phi_{t+1}     &amp;\le  \frac{1}{2\eta_t} (D_{t}^2 - D_{t+1}^2 -  \frac{1}{2}\|x^{t+1} - x^{*}\| )+ \frac{\beta}{2} \| x^{t+1} - x^t \|^2 \\<br>
&amp;\le \frac{1}{2\eta_t} (D_{t}^2 - D_{t+1}^2) - \| x^{t+1} - x^t \|^2 (\frac{1}{2\eta_t} - \frac{\beta}{2})
\end{align*}
Set  $\eta_t = \frac{c}{\beta} $  where, $c\in [0,1]$ . We get($0 \ge k\le 1) $
$$ (\frac{1}{2\eta_t} - \frac{\beta}{2}) = \frac{\beta}{2} (\frac{1}{k} - 1)\ge 0  $$</p>
<p>\begin{align*}
\sum_{t=0}^T \Phi_{t+1} &amp;\le \sum_{t=0}^T \frac{\beta}{2c} (D_{t}^2 - D_{t+1}^2)\\<br>
\frac{1}{T} \sum_{t=0}^T \Phi_{t+1}  &amp;\le \frac{\beta}{2cT} (D_{0}^2 - D_{T+1}^2)
\end{align*}
Applying Jensen's inequality because $\Phi_t$ is convex
$$f(\frac{\sum_{t=0}^T x^t}{T}) \le \sum_{t=0}^T  \frac{f(x^t) }{T} - f(x^{*}) \le \frac{\beta}{2cT} (D_{0}^2 ) $$</p>
<p>For both the strongly convex and the strong smooth functions, we have seen the average selector. Is it possible that some other selector can give us better bounds ? Selector refers to the particular way of choosing the $x$, which we want the function to return.</p>

    </div>

    



<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/academic/">academic</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://amartya18x.github.io/post/strconvex/&amp;text=Strong%20Convexity%20and%20Strong%20Smoothness" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://amartya18x.github.io/post/strconvex/&amp;t=Strong%20Convexity%20and%20Strong%20Smoothness" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Strong%20Convexity%20and%20Strong%20Smoothness&amp;body=https://amartya18x.github.io/post/strconvex/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://amartya18x.github.io/post/strconvex/&amp;title=Strong%20Convexity%20and%20Strong%20Smoothness" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Strong%20Convexity%20and%20Strong%20Smoothness%20https://amartya18x.github.io/post/strconvex/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://amartya18x.github.io/post/strconvex/&amp;title=Strong%20Convexity%20and%20Strong%20Smoothness" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  






  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hue776a334b902d7828bf5c7a8aa11ccb3_924185_270x270_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://amartya18x.github.io/">Amartya Sanyal</a></h5>
      <h6 class="card-subtitle">DPhil (PhD) Student in Computer Science</h6>
      <p class="card-text">I am interested in  understanding the behaviours of deep neural networks and designing practical algorithms to avoid some of its unwanted artifacts.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:amartya18x@gmail.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/AmartyaSanyal" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=cRLqsyYAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/amartya18x" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/files/cv.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>




<section id="comments">
  
    
<div id="disqus_thread"></div>
<script>
  let disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); s.async = true;
    s.src = 'https://' + "amartyasanyal" + '.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
</section>






  
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.4.4/mermaid.min.js" integrity="sha256-JEqEejGt4tR35L0a1zodzsV0/PJ6GIf7J4yDtywdrH8=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    <script id="dsq-count-scr" src="https://amartyasanyal.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.a8d7005002cb4a052fd6d721e83df9ba.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
