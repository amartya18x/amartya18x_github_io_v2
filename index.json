[{"authors":["admin"],"categories":null,"content":"#Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\n#Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"#Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\n#Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet.","tags":null,"title":"Amartya Sanyal","type":"authors"},{"authors":["Amartya Sanyal","Philip H.S. Torr","Puneet K Dokania"],"categories":null,"content":"","date":1576713600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576713600,"objectID":"b8602d83a6549b38cd514d51c854fe30","permalink":"/publication/stable/","publishdate":"2019-12-19T00:00:00Z","relpermalink":"/publication/stable/","section":"publication","summary":"Exciting new work on generalization bounds for neural networks (NN) given by Bartlett et al. (2017); Neyshabur et al. (2018) closely depend on two parameter- dependant quantities: the Lipschitz constant upper bound and the stable rank (a softer version of rank). Even though these bounds typically have minimal practical utility, they facilitate questions on whether controlling such quantities together could improve the generalization behaviour of NNs in practice. To this end, we propose stable rank normalization (SRN), a novel, provably optimal, and computationally efficient weight-normalization scheme which minimizes the stable rank of a linear operator. Surprisingly we find that SRN, despite being non-convex, can be shown to have a unique optimal solution. We provide extensive analyses across a wide variety of NNs (DenseNet, WideResNet, ResNet, Alexnet, VGG), where applying SRN to their linear layers leads to improved classification accuracy, while simultaneously showing improvements in genealization, evaluated empirically usingâ€”(a) shattering experiments (Zhang et al., 2016); and (b) three measures of sample complexity by Bartlett et al. (2017), Neyshabur et al. (2018), \u0026 Wei \u0026 Ma. Additionally, we show that, when applied to the discriminator of GANs, it improves Inception, FID, and Neural divergence scores, while learning mappings with low empirical Lipschitz constant.","tags":null,"title":"Stable Rank Normalization for Improved Generalization in Neural Networks and GANs","type":"publication"},{"authors":["Amartya Sanyal","Matt kusner","Adria Gascon","Varun Kanade"],"categories":null,"content":"","date":1525996800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525996800,"objectID":"e83a85ba8e4e8a99b77242172592406c","permalink":"/publication/epaas/","publishdate":"2018-05-11T00:00:00Z","relpermalink":"/publication/epaas/","section":"publication","summary":"Machine learning methods are widely used for a variety of prediction problems. Prediction as a service is a paradigm in which service providers with technological expertise and computational resources may perform predictions for clients. However, data privacy severely restricts the applicability of such services, unless measures to keep client data private (even from the service provider) are designed. Equally important is to minimize the amount of computation and communication required between client and server. Fully homomorphic encryption offers a possible way out, whereby clients may encrypt their data, and on which the server may perform arithmetic computations. The main drawback of using fully homomorphic encryption is the amount of time required to evaluate large machine learning models on encrypted data. We combine ideas from the machine learning literature, particularly work on binarization and sparsification of neural networks, together with algorithmic tools to speed-up and parallelize computation using encrypted data.","tags":null,"title":"TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service","type":"publication"},{"authors":["Amartya Sanyal","Varun Kanade","Philip H.S. Torr"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"01fd45ce39bee703e80ca5749009ef66","permalink":"/publication/lr_layer/","publishdate":"2018-04-01T00:00:00Z","relpermalink":"/publication/lr_layer/","section":"publication","summary":"A key feature of neural networks, particularly deep convolutional neural networks, is their ability to 'learn' useful representations from data. The very last layer of a neural network is then simply a linear model trained on these 'learned' representations. Despite their numerous applications in other tasks such as classification, retrieval, clustering etc., a.k.a. transfer learning, not much work has been published that investigates the structure of these representations or whether structure can be imposed on them during the training process. In this paper, we study the dimensionality of the learned representations by models that have proved highly succesful for image classification. We focus on ResNet-18, ResNet-50 and VGG-19 and observe that when trained on CIFAR10 or CIFAR100 datasets, the learned representations exhibit a fairly low rank structure. We propose a modification to the training procedure, which further encourages low rank representations of activations at various stages in the neural network. Empirically, we show that this has implications for compression and robustness to adversarial examples.","tags":null,"title":"Learning Low Rank Representations","type":"publication"},{"authors":["Amartya Sanyal","Purushottam Kar","Pawan Kumar","Sanjay Chawla"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"7e4a081efbce15b3b73a30bcdc6210eb","permalink":"/publication/non_dec/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/publication/non_dec/","section":"publication","summary":"We present a class of algorithms capable of directly training deep neural networks with respect to task-specific performance measures such as the F-measure and the Kullback-Leibler diver- gence that are structured and non-decomposable. This presents a departure from standard deep learning techniques that typically use squared or cross-entropy loss functions, that are decompos- able, to train the networks.","tags":null,"title":"Optimizing Non-decomposable Measures with Deep Networks","type":"publication"}]