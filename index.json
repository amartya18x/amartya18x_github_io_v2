[{"authors":["admin"],"categories":null,"content":"Biography I am Amartya Sanyal, currently a D.Phil(PhD) student at the Department of Computer Science at the University of Oxford where I am advised by Prof. Varun Kanade and Prof. Phil H.S. Torr. I completed by Bachelor Of Technology in Computer Science And Engineering from the Indian Institute of Technology, Kanpur in 2017.\nCurrently, my research interests span theoretical and empirical investigation on the reliability of modern deep learning methods in aspects of robustness and generalization in the presence of noise. In particular, I look at the importance of proper regularizations and representation learning that can help to effectively improve these properties. My research also looks at improving privacy, calibration and computational efficiency of deep learning methods.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1596630979,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://amartya18x.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Biography I am Amartya Sanyal, currently a D.Phil(PhD) student at the Department of Computer Science at the University of Oxford where I am advised by Prof. Varun Kanade and Prof. Phil H.S. Torr. I completed by Bachelor Of Technology in Computer Science And Engineering from the Indian Institute of Technology, Kanpur in 2017.\nCurrently, my research interests span theoretical and empirical investigation on the reliability of modern deep learning methods in aspects of robustness and generalization in the presence of noise.","tags":null,"title":"","type":"authors"},{"authors":["Amartya Sanyal"],"categories":null,"content":"","date":1596796200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597053058,"objectID":"acf81a2ada591db7633d8808d696e825","permalink":"https://amartya18x.github.io/talk/iitkbenign/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/iitkbenign/","section":"talk","summary":"Talk at the department of Computer Science, IIT Kanpur.","tags":[],"title":"How Benign is Benign Overfitting ?","type":"talk"},{"authors":["Amartya Sanyal","Varun Kanade","Puneet K. Dokania","Philip H.S. Torr"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596630979,"objectID":"9d4ae66fd675968f8d1d37e81c68bbfa","permalink":"https://amartya18x.github.io/publication/benign/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/benign/","section":"publication","summary":"We investigate two causes for adversarial vulnerability in deep neural networks: bad data and (poorly) trained models. When trained with SGD, deep neural networks essentially achieve zero training error, even in the presence of label noise, while also exhibiting good generalization on natural test data, something referred to as benign overfitting. However, these models are vulnerable to adversarial attacks. We identify label noise as one of the causes for adversarial vulnerability, and provide theoretical and empirical evidence in support of this. Surprisingly, we find several instances of label noise in datasets such as MNIST and CIFAR, and that robustly trained models incur training error on some of these, i.e. they don't fit the noise. However, removing noisy labels alone does not suffice to achieve adversarial robustness. Standard training procedures bias neural networks towards learning 'simple' classification boundaries, which may be less robust than more complex ones. We observe that adversarial training does produce more complex decision boundaries. We conjecture that in part the need for complex decision boundaries arises from sub-optimal representation learning. By means of simple toy examples, we show theoretically how the choice of representation can drastically affect adversarial robustness.","tags":null,"title":"How Benign is Benign Overfitting?","type":"publication"},{"authors":["Pau De Jorge","Amartya Sanyal","Harkirat Behl","Philip H.S. Torr","Gregory Rogez","Puneet K. Dokania"],"categories":null,"content":"","date":1593388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596630979,"objectID":"178b2392993b91c699952ee256bd8e82","permalink":"https://amartya18x.github.io/publication/force/","publishdate":"2020-06-29T00:00:00Z","relpermalink":"/publication/force/","section":"publication","summary":"We investigate two causes for adversarial vulnerability in deep neural networks: bad data and (poorly) trained models. When trained with SGD, deep neural networks essentially achieve zero training error, even in the presence of label noise, while also exhibiting good generalization on natural test data, something referred to as benign overfitting. However, these models are vulnerable to adversarial attacks. We identify label noise as one of the causes for adversarial vulnerability, and provide theoretical and empirical evidence in support of this. Surprisingly, we find several instances of label noise in datasets such as MNIST and CIFAR, and that robustly trained models incur training error on some of these, i.e. they don't fit the noise. However, removing noisy labels alone does not suffice to achieve adversarial robustness. Standard training procedures bias neural networks towards learning 'simple' classification boundaries, which may be less robust than more complex ones. We observe that adversarial training does produce more complex decision boundaries. We conjecture that in part the need for complex decision boundaries arises from sub-optimal representation learning. By means of simple toy examples, we show theoretically how the choice of representation can drastically affect adversarial robustness.","tags":null,"title":"Progressive Skeletonization: Trimming more fat from a network at initialization","type":"publication"},{"authors":["Amartya Sanyal"],"categories":null,"content":"","date":1587999600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596630979,"objectID":"5935c31e45e4cd960e2b535b9cfed0c0","permalink":"https://amartya18x.github.io/talk/iclr2020/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/iclr2020/","section":"talk","summary":"Spotlight Talk at the International Conference on Learning Representations, 2020","tags":[],"title":"Stable Rank Normalization for Improved Generalization in Neural Networks and GANs","type":"talk"},{"authors":["Jishnu Mukhoti","Viveka Kulharia","Amartya Sanyal","Stuart Golodetz","Philip H.S. Torr","Puneet K. Dokania"],"categories":null,"content":"","date":1582243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596630979,"objectID":"659fc4181b1a8a19685d4754c0571da9","permalink":"https://amartya18x.github.io/publication/calibration/","publishdate":"2020-02-21T00:00:00Z","relpermalink":"/publication/calibration/","section":"publication","summary":"Miscalibration -- a mismatch between a model's confidence and its correctness -- of Deep Neural Networks (DNNs) makes their predictions hard to rely on. Ideally, we want networks to be accurate, calibrated and confident. We show that, as opposed to the standard cross-entropy loss, focal loss (Lin et al., 2017) allows us to learn models that are already very well calibrated. When combined with temperature scaling, whilst preserving accuracy, it yields state-of-the-art calibrated models. We provide a thorough analysis of the factors causing miscalibration, and use the insights we glean from this to justify the empirically excellent performance of focal loss. To facilitate the use of focal loss in practice, we also provide a principled approach to automatically select the hyperparameter involved in the loss function. We perform extensive experiments on a variety of computer vision and NLP datasets, and with a wide variety of network architectures, and show that our approach achieves state-of-the-art accuracy and calibration in almost all cases.","tags":null,"title":"Calibrating Deep Neural Networks using Focal Loss","type":"publication"},{"authors":["Amartya Sanyal","Puneet K. Dokania","Varun Kanade","Philip H.S. Torr"],"categories":null,"content":"In this paper ( Full Paper here), we investigate the relation of the intrinsic dimension of the representation space of deep networks with its robustness.\nObjective (TL;DR)  Classical machine learning uses dimensionality reduction techniques like PCA to increase the robustness as well as compressibility of data representations.      While, modern neural networks are highly successful in a large number of tasks, they have been shown to be vulnerable to input perturbations. We draw inspiration from classical dimensionality reduction techniques and introduce an algorithm to learn robust low rank features without changing the NN architectures.   Training uses an additional LR Layer, which encourages the Neural Network features to be low rank. During inference, this extra layer is thrown away and the NN features are already low rank.     To do this, we introduce an easy-to-implement, end-to-end trainable, scalable regularizer (LR Layer) that enforces low rank structure in the representation space of deep networks.\n  We conduct a variety of experiments to show that the resultant network is largely robust to adversarial and random perturbations without any adversarial training.\n  In addition, the low intrinsic dimension also means that the representations (and the model) can be compressed significantly (almost 400x) without significant loss in accuracy.\n  TL;SR Low Rank Representations  For most models trained in a supervised fashion, the vector of activations in the penultimate layer~(or a layer close to the penultimate layer) is a learned representation of the raw input.   The remarkable success of DNNs is primarily attributed to the discriminative quality of this learned representation space. However, despite their impressive performance, DNNs are known to be brittle to input perturbations. This raises concerns regarding the robustness of the factors captured by the learned representation space of DNNs. But we know that the factors captured by dimensionality reduction techniques, while being discriminative, are robust to input perturbations. This motivates the thesis behind this work:\n If we enforce DNNs to learn representations that lie in a low-dimensional subspace (for the entire dataset), we would obtain more robust classifiers while preserving their discriminative power.   Precisely, we propose a low-rank regularizor (LR) that\n does not put any restriction on the network architecture. is end-to-end trainable is efficient in that it allows mini-batch training  Problem Formulation Consider $f: \\mathbb{R}^p \\rightarrow \\mathbb{R}^k$ to be a feed-forward multilayer NN that maps $p$ dimensional input $x$ to a $k$ dimensional output $y$. We can decompose this into two sub-networks, one consisting of the layers before the $\\ell^{\\it th}$ layer and one after i.e. $f(x) = f_\\ell^{+}(f^{-}_\\ell(x; \\phi) ; \\theta)$, where $f^{-}_\\ell (.;\\phi)$, parameterized by $\\phi$, represents the part of the network up to layer $\\ell$ and, $f^{+}_\\ell(.;\\theta)$ represents the part of the network thereafter. With this notation, the $m$ dimensional representation (or the activations) of any layer $\\ell$ can simply be written as $a = f^{-}_\\ell(x; \\phi) \\in \\mathbb{R}^m$.\nLet $X = \\{x_i\\}_{i=1}^n$ and $Y = \\{y_i\\}_{i=1}^n$ be the set of inputs and outputs of a given training dataset. By slight abuse of notation, we define $A_\\ell = f^{-}_\\ell(X; \\phi) =[a_1,\\cdots,a_n]^\\top\\in \\mathbb{R}^{n \\times m}$ to be the activation matrix of the entire dataset, so that $a_i$ is the activation vector of the $i$-th sample. Note that for most practical purposes $n\\gg m$.\nXXfℓ-( ;φ)fℓ-( ;φ)AℓAℓfℓ+( ;θ)fℓ+( ;θ)OUTPUTOUTPUTDataDataActivationsActi\u0026hellip;ffViewer does not support full SVG 1.1\nIn this setting, the problem of learning low-rank representations can be formulated as a constrained optimization problem as follows: \\begin{align} \\label{eq:opt_prob} \\min_{\\theta, \\phi}\\mathcal{L}(X, Y; \\theta, \\phi),~\\text{s.t.}~~\\mathrm{rank}(A_\\ell) = r,\\end{align} where $\\mathcal{L}(.)$ is the loss function and $r \u0026lt; m$ is the desired rank of the representations at layer $\\ell$. The rank $r$ is a hyperparameter. However, there are a few problems with this formulation.\nFirst, the rank constraint is on a matrix\n whose size scales with dataset size. which is not a parameter; and it is not immediately clear if a Tikhonov regularizor exists that can achieve this.  Second, minimizing the restriction of $A_\\ell$ on a minibatch can result in orthogonal low rank spaces for each minibatch thus having a high dimensional subspace when all the minibatches are combined.\nTo mitigate these issues, we augment the initial problem as follows:\nAugmented problem (our low rank regularizer) $$ \\min_{\\theta, \\phi, W, b} \\mathcal{L}(X, Y; \\theta, \\phi) + \\mathcal{L}_c(A_\\ell; W,b) + \\mathcal{L}_n(A_\\ell)$$ $$\\text{s.t.,} W\\in \\mathbb{R}^{m\\times m}, \\mathrm{rank}(W) = r,~ b\\in \\mathbb{R}^m,A=f^{-}_\\ell(X; \\phi) $$ where, $$ \\underbrace{\\mathcal{L}_c(A; W, b) = \\frac{1}{n} \\sum_{i=1}^{n} \\Big\\|W^\\top(a_i+b) - (a_i+b)\\Big\\|_2^2}_{\\Large\\text{Projection Loss}}$$ $$ \\text{and} \\quad \\underbrace{\\mathcal{L}_n(A) = \\frac{1}{n}\\sum_{i=1}^n \\Big|1 - \\|a_i\\| \\Big|}_{\\Large \\text{Norm Loss}}$$\nProjection Loss: Minimizing the projection loss $\\mathcal{L}_c$ ensures that the affine low-rank mappings ($AW$) of the activations are close to the original ones i.e. $AW \\approx A$. As $W$ is low-rank, recalling sub-multiplicity of rank - $\\mathrm{rank}(AW)\\le \\mathrm{min}(\\mathrm{rank}(A),\\mathrm{rank}(W))$, $AW$ is also low-rank; thus implicitly~(due to $AW\\approx A$) it forces the original activations $A$ to be low-rank. The bias $b$ allows for the activations to be translated before projection.\nNorm Loss: However note that setting $A$ and $b$ close to zero trivially minimizes $\\mathcal{L}_c$, especially when the activation dimension is large. We observed this to happen in practice as it is easier for the network to learn $\\phi$ such that the activations and the bias are very small in order to minimize $\\mathcal{L}_c$. To prevent this, we use $\\mathcal{L}_n$ that acts as a norm constraint on the activation vector to keep the activations sufficiently large.\nIntuitively, we learn a virtual layer i.e.($W,b$) during training that does two things.\n  Learns a very low dimensional subspace that captures a large fraction of the information present in the representations/activations of the network on the training data.\n  Learns to encourage the representations/activations to lie as much as possible entirely on this low dimensional subspace.\n  Optimizing the Loss We optimize the loss using an alternate minimization scheme. During forward pass, the loss from the three components $\\mathcal{L}, \\mathcal{L}_c,\\mathcal{L}_n$ are back-propagated through the network. Every $10$ iteration, $W$ is rank thresholded using column-sampled ensembled Nyström SVD, which is essentially an approximate SVD.\n  Forward and backward pass through our regularizor.   Adversarial Robustness of our method We recall that adversarial perturbations are well crafted~(almost imperceptible) input perturbations that, when added to a clean input, flips the prediction of the model on the input to an incorrect one.\n  An example of an image of a pig that is initially correctly classified by a classifier. On adding a small imperceptible perturbation, the same classifier mis-classifies it as an airliner. [a]   We look at the adversarial robustness of our model as compared to a vanilla model i.e. one without our regularizor; and also with some other methods that impose constraints on the parameter space. We test it against two main adversaries -\n one that is computationally constrained where we measure the success of the adversarial attack for different computational budgets, computationally unconstrained where we measure the amount of perturbation necessary for $99%$ mis-classification.  Computationally Constrained Adversary\nIn the table below, we measure the adversarial accuracy of a ResNet50 model trained on the CIFAR10 dataset against an untargeted $\\ell_{\\infty}$ PGD adversary with the computational constraints (i.e. attack budgets measured in terms of $\\ell_\\infty$ radius and att. steps. Higher these values, stronger is the adversary.) The table shows that our Low Rank regularizer has a much higher test accuracy than any of the other methods used.\nResNet50 trained on CIFAR10. Low Rank uses our regularizor; SNIP [1] is a pruning technique to to enforce sparsity in parameters and SRN[2] is a technique to introduce a soft low rank structure in the parameter space.  Algorithm Adversarial Test Accuracy  $\\ell_\\infty$ radius 8\u0026frasl;255 10\u0026frasl;255 16\u0026frasl;255 20\u0026frasl;255  Att. steps 7 20 7 20 7 20 7 20    Vanilla  43.1   31.0   38.5   21.8   31.2   7.8   28.9   4.5   SNIP [1]   29.4   14.5   25.0   8.0   18.5   1.3   16.2   0.4   SRN [2]   47.8  37.6  44.4  31.4  39.8  21.3  37.5  18.4  Low Rank (Ours)  $\\mathbf{79.1}$   $\\mathbf{78.5}$   $\\mathbf{78.6}$   $\\mathbf{78.1}$   $\\mathbf{77.9}$   $\\mathbf{77.0}$   $\\mathbf{77.1}$   $\\mathbf{76.6}$     Computationally UnConstrained Adversary (Deepfool) [3]\nWe also look at the amount of adversarial perturbation that is necessary to fool our classifier as opposed to a vanilla model. Measured numerically, ours requires an order of magnitude more than normal models. Please refer to our paper for the exact values. If visualized, the images appear as below and the adversarial images for teh Low-Rank model (LR) are significantly more different from the original model than those for the vanilla model.\n   Original Image Vanilla Model Low Rank Model                            Noise Stability We explain this robustness by showing that the noise-stability of the low rank representations are much more than those of vanilla models. More specifically, we show two different experiments\n  Random Pixel Perturbation:  When a random subset (each pixel is chosen iid with probability $p$) of the pixels of a image are perturbed with a random multi-variate gaussian noise, the test accuracy of our low-rank models drops much slower than that of a vanilla model.     Pert. Prob. $p$ $0.4$ $0.6$ $0.8$ $1.0$     Vanilla $69.7$ $26.1$ $12.6$ $11.3$   Low-Rank (Ours) $\\mathbf{75.1}$ $\\mathbf{34.2}$ $\\mathbf{15.8}$ $\\mathbf{13.0}$    2. Noise-Stability of Representations: We measure the ratio of the norm of the perturbation induced in the representation space with to the norm of the perturbation in the input space for adversarial perturbations. In the following diagram, x-axis measures $$\\text{x-axis}:\\dfrac{\\|\\delta\\|_2^2}{\\|x\\|_2^2}\\quad \\text{y-axis}:\\dfrac{\\|f_\\ell^{-}(x+\\delta) - f_\\ell^{-}(x)\\|_2^2}{\\|f_\\ell^{-}(x)\\|_2^2}$$ White box attacks are attacks where the perturbation is constructed using the model to be attacked, black box models are attacks where the vanilla model is used to construct the attack.\n    White Box Attack Black Box Attack                 Further observations in the paper  Layer Cusion: Arora et. al [4] define a quantity called layer cushion, which is intuitively the reciprocal of noise-sensitivity to random gaussian noise measured on the real dataset. We measure this quantity for all our networks and show that this quantity is much higher for our networks. Compressing Representations: Due to the low intrinsic dimension of the low rank representations, even after aggressive compression ($400x$) outhe low rank model looses only $6\\%$ in accuracy as opposed to the vanilla model which looses more than $27\\%$. Compressing Models: We also throw away the latter parts of the model (after the low rank representation) and train a small linear model ($8M$ replaced with $160k$ parameters) yielding less than $1\\%$ drop in accuracy. Discriminative Representations: When visualized with PCA (and colored according to classes), the low rank model yields representations that are more discriminative in nature in the sense that a low dimensional lienar classifier can classify it with a much larger margin than for vanilla models.     Vanilla Model Low Rank Model              For the full paper refer to https://arxiv.org/abs/1804.07090.\n[a] Picture taken from https://medium.com/@smkirthishankar/the-unusual-effectiveness-of-adversarial-attacks-e1314d0fa4d3 [1] Lee, Namhoon, Thalaiyasingam Ajanthan, and Philip HS Torr. \"Snip: Single-shot network pruning based on connection sensitivity.\" International Conference on Learning Representations (2019). [2] Sanyal, Amartya, Philip HS Torr, and Puneet K. Dokania. \"Stable Rank Normalization for Improved Generalization in Neural Networks and GANs.\" International Conference on Learning Representations (2020). [3] Moosavi-Dezfooli, Seyed-Mohsen, Alhussein Fawzi, and Pascal Frossard. \"Deepfool: a simple and accurate method to fool deep neural networks.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. [4] Arora, Sanjeev, et al. \"Stronger generalization bounds for deep nets via a compression approach.\" arXiv preprint arXiv:1802.05296 (2018). ","date":1579783223,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596630979,"objectID":"ac5fb05d6df6441d0bdb16d7e7e6b8d4","permalink":"https://amartya18x.github.io/post/lr_layer/","publishdate":"2020-01-23T12:40:23Z","relpermalink":"/post/lr_layer/","section":"post","summary":"In this paper ( Full Paper here), we investigate the relation of the intrinsic dimension of the representation space of deep networks with its robustness.\nObjective (TL;DR)  Classical machine learning uses dimensionality reduction techniques like PCA to increase the robustness as well as compressibility of data representations.      While, modern neural networks are highly successful in a large number of tasks, they have been shown to be vulnerable to input perturbations.","tags":null,"title":"Robustness via Deep Low-Rank Representations","type":"post"},{"authors":["Amartya Sanyal","Varun Kanade","Puneet K. Dokania","Philip H.S. Torr"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582198251,"objectID":"01fd45ce39bee703e80ca5749009ef66","permalink":"https://amartya18x.github.io/publication/lr_layer/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/lr_layer/","section":"publication","summary":"We investigate the effect of the dimensionality of the representations learned in Deep Neural Networks (DNNs) on their robustness to input perturbations, both adversarial and random. To achieve low dimensionality of learned representations, we propose an easy-to-use, end-to-end trainable, low-rank regularizer (LR) that can be applied to any intermediate layer representation of a DNN. This regularizer forces the feature representations to (mostly) lie in a low-dimensional linear subspace. We perform a wide range of experiments that demonstrate that the LR indeed induces low rank on the representations, while providing modest improvements to accuracy as an added benefit. Furthermore, the learned features make the trained model significantly more robust to input perturbations such as Gaussian and adversarial noise (even without adversarial training). Lastly, the low-dimensionality means that the learned features are highly compressible; thus discriminative features of the data can be stored using very little memory. Our experiments indicate that models trained using the LR learn robust classifiers by discovering subspaces that avoid non-robust features. Algorithmically, the LR is scalable, generic, and straightforward to implement into existing deep learning frameworks.","tags":null,"title":"Robustness via Deep Low-Rank Representations","type":"publication"},{"authors":["Amartya Sanyal","Philip H.S. Torr","Puneet K Dokania"],"categories":null,"content":"","date":1576713600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582029119,"objectID":"b8602d83a6549b38cd514d51c854fe30","permalink":"https://amartya18x.github.io/publication/stable/","publishdate":"2019-12-19T00:00:00Z","relpermalink":"/publication/stable/","section":"publication","summary":"Exciting new work on generalization bounds for neural networks (NN) given by Bartlett et al. (2017); Neyshabur et al. (2018) closely depend on two parameter- dependant quantities: the Lipschitz constant upper bound and the stable rank (a softer version of rank). Even though these bounds typically have minimal practical utility, they facilitate questions on whether controlling such quantities together could improve the generalization behaviour of NNs in practice. To this end, we propose stable rank normalization (SRN), a novel, provably optimal, and computationally efficient weight-normalization scheme which minimizes the stable rank of a linear operator. Surprisingly we find that SRN, despite being non-convex, can be shown to have a unique optimal solution. We provide extensive analyses across a wide variety of NNs (DenseNet, WideResNet, ResNet, Alexnet, VGG), where applying SRN to their linear layers leads to improved classification accuracy, while simultaneously showing improvements in genealization, evaluated empirically using—(a) shattering experiments (Zhang et al., 2016); and (b) three measures of sample complexity by Bartlett et al. (2017), Neyshabur et al. (2018), \u0026 Wei \u0026 Ma. Additionally, we show that, when applied to the discriminator of GANs, it improves Inception, FID, and Neural divergence scores, while learning mappings with low empirical Lipschitz constant.","tags":null,"title":"Stable Rank Normalization for Improved Generalization in Neural Networks and GANs","type":"publication"},{"authors":["Amartya Sanyal","Matt kusner","Adria Gascon","Varun Kanade"],"categories":null,"content":"","date":1525996800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582029119,"objectID":"e83a85ba8e4e8a99b77242172592406c","permalink":"https://amartya18x.github.io/publication/epaas/","publishdate":"2018-05-11T00:00:00Z","relpermalink":"/publication/epaas/","section":"publication","summary":"Machine learning methods are widely used for a variety of prediction problems. Prediction as a service is a paradigm in which service providers with technological expertise and computational resources may perform predictions for clients. However, data privacy severely restricts the applicability of such services, unless measures to keep client data private (even from the service provider) are designed. Equally important is to minimize the amount of computation and communication required between client and server. Fully homomorphic encryption offers a possible way out, whereby clients may encrypt their data, and on which the server may perform arithmetic computations. The main drawback of using fully homomorphic encryption is the amount of time required to evaluate large machine learning models on encrypted data. We combine ideas from the machine learning literature, particularly work on binarization and sparsification of neural networks, together with algorithmic tools to speed-up and parallelize computation using encrypted data.","tags":null,"title":"TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service","type":"publication"},{"authors":["Amartya Sanyal","Purushottam Kar","Pawan Kumar","Sanjay Chawla"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582198846,"objectID":"7e4a081efbce15b3b73a30bcdc6210eb","permalink":"https://amartya18x.github.io/publication/non_dec/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/publication/non_dec/","section":"publication","summary":"We present a class of algorithms capable of directly training deep neural networks with respect to task-specific performance measures such as the F-measure and the Kullback-Leibler diver- gence that are structured and non-decomposable. This presents a departure from standard deep learning techniques that typically use squared or cross-entropy loss functions, that are decompos- able, to train the networks.","tags":null,"title":"Optimizing Non-decomposable Measures with Deep Networks","type":"publication"},{"authors":null,"categories":null,"content":"I don\u0026rsquo;t remember where I heard this arguement but it was a pretty interesting one and I couldn\u0026rsquo;t remember the entire reasoning and so as I began to think about it, I figured some of it out and I believe it is pretty reasonable. We have all argued that neural networks are difficult to optimize because they are non-convex(or non-concave) for that matter. What we don\u0026rsquo;t talk about so much is why is it that they are non-convex. It probably has simpler answers where you will argue something about the non-linearities involved. This post has to do with a much more interesting answer.\nTo understand the reasoning, we have to argue that if the function has atleast two local minimas such that their mid-point is not a local minima, the function is non-convex. In formal terms, $$\\text{If }\\exists x,y \\in Dom(f) \\hspace{5pt }s.t.\\hspace{5pt} g(x) = g(y) = 0 $$ and $$\\forall z\\in [x,y] \\text{ s.t. } g(z) \\neq 0 $$ where $g(x)$ is a subgradient at $x$ the function is non-convex.\nThis is pretty simple to follow from the Mid-point value theorem and the definition of convexity.\nConsider a neural network $F(\\cdot)$ with a few layers and name three layers $L_1, L_2$ and $ L_3$ Consider nodes $a$ and $b$ in $L_1$ and $c$ and $d$ in $L_2$.Let the parameters connecting node $i$ to $j$ be called $w_{ij}$.\nThee arguement to the function can be thought of as $A=[\\cdots w_{ac}, w_{ad}, \\cdots , w_{bc}, w_{bd}\\cdots]$ and the function evaluation as $f(A)$. Consider $B=[\\cdots w_{ad}, w_{ac},\\cdots, w_{bd}, w_{bc}, \\cdots]$. I can claim that there exists a $B$ i.e. where $ w_{ad}, w_{ac}$ and $ w_{bd}, w_{bc} $ are swapped respectively, there exists an ordering of the other weights(permute the other edges) such that the function evaluation remains same. It is pretty easy to see how it can be true.\n Look at the following picture where the reg edges come into one node and the green into another.\n    Before permutation!   For reference consider\n The first red edge coming out of the top left node as $w_{ac}$ The gree edge out of the same node as $w_{ad}$. The red edge out of the second node in the same layer as $w_{bc}$ The green edge out of the second node as $ w_{bd} $.  Connect all edges coming in to $L_2$ from $L_1$ into node $d$ to node $c$ and vice versa and then shift the origin of all outgoing edges from node $c$ to $L_3$ to node $d$ and vice versa. What you have acheived is the network that you would have achieved by holding the two nodes by your hand and manually exchanging their positions while keeping the connecting edges intact. And Voila! now you have $A, B \\in dom(F)$ where $F(A) = F(B) $.\n Now look at them after the edges are shifted.   After permutation!    The natural doubt you might suddenly have is What about a linear network ? . Well, it is a well known fact that such a function can be stated as $F(x; A, b) = Ax + b$ for some matrix $A$ and vector $B$. And as you know, linear layers do not have a maxima or a minima and hence the whole assumption we made about having $g(x) = 0$ i.e. zero subgradients, do not hold. Thus, it does not contradict the statement given earlier.\nWe will also need to understand how it holds the other point about there existing some point between the local optimas such that it does not have the same value as the optima. To understand this lets look at simplified representation of $F(\\cdot)$. Instead, of taking the weights of the edges to be the arguements of $F(\\cdot)$, let the nodes be the arguements.\n$$F(n_{1,1}, n_{1, 2}, \\cdots, n_{3,1}\\cdots)$$ where $n_{i, j}$ is the $j^{th}$ node in $i^{th}$ layer. $$n_{i,j} = \\sum_{j = 1}^k w_{j,k}^{i-1}\\sigma (n_{i-1, k})$$ where $w_{j,k}^{i-1}$ is the weight connecting the $k^{th}$ node in ${i-1}^{th}$ layer to the $j^{th}$ node in layer $i$.\nNote that the transformation from the earlier definition to this definition is many-to-one and not one-to-one, which is easy to guess because of the reduction in the number of parameters.\nNow, it is also easy to observe that if we permute $n_{1,1}$ amd $n_{1,2}$, $F(\\cdot)$ doesn\u0026rsquo;t change it\u0026rsquo;s value.To see how this permuation relates to the original neural network, we barely need to change the weights in the original function as mentioned above(In the diagram interchange the two nodes in the second layer) i.e. the permutation of the weights mentioned above is equaivalent to this permutation.\n What we can infer from this is that for each unique optimal value, there exists $\\prod_{i=1}^k n_k!$ points in the parameter space that can achieve that local optima, where $n_k$ is the number of nodes in the $k^{th}$ layer. Turns out that the number of saddle points are exponentially more!\n The above statement however holds true only when the values of the $n_k$ nodes in that layer are also distinct but I guess the idea is clear and it is more of discrete mathematics.\n What this might look like is shown below. Note that this is not the loss function of a neural network but rather a function called Rastrigin function taken from wikipedia.\n   Many many optimas!   To observe that the points between the local minimas are suboptimal, you will probably have to figure something out more rigorous as I do not have a very tight arguement, but lets look at a very interesting intuition.\nLet\u0026rsquo;s assume that the function is convex. This would allow the function to be hit by jensen\u0026rsquo;s inequaltiy which states that for a convex function $g(\\cdot)$ $$g(\\frac{1}{k}\\sum_{i = 1}^k x_i) \\le \\frac{1}{k} \\sum_i g(x_i)$$ We know that the nodes are permutable within a layer. Let there be $M$ layers and $N_i$ nodes in each layer. We can thus get $$\\prod_{i=1}^M N_i !$$ total permutations(Doesn\u0026rsquo;t really matter if they are unique or not). If you apply jensen\u0026rsquo;s inequality to this, what we will have is that the function evaluation at the average is less than or equal to the average of these optimal points, which are all equal and hence what we get is that the evaluation at the average point is also optimal(If it is not, the function is not convex).\nBut notice that we are actually averaging all the permutations possible in a layer and thus what we will get is a set of arguements such that the nodes that belong to the same layer have the same value. To understand this look at the matrices below and assume one layer corresponds to a column. The two matrices are thus independant permutations of the two columns.[ M_1= \\begin{bmatrix} 1 \u0026amp; 2 \\\\\\\n3 \u0026amp; 4 \\end{bmatrix} ]\n[ M_2= \\begin{bmatrix} 3 \u0026amp; 4 \\\\\\\n1 \u0026amp; 2 \\end{bmatrix} ]\n[ \\frac{M_1 + M_2}{2}= \\begin{bmatrix} 2 \u0026amp; 3 \\\\\\\n2 \u0026amp; 3 \\end{bmatrix} ] What this effectively means is that if the neural network was indeed convex, there would be a value which could be given to all nodes in a layer and yet the network would have represented a local optima. This is higly absurd and is very apparent if you consider a classification network where the final outputs should not have the same value in all nodes given some value in the input layer(which is also a part of the function arguement).\nTo generalize this to any neural network, you will have to look at the activation functions and their properties. The claim you will be trying to verify is that the average of two permutations of an optimal point need not be optimal.\nI guess this was an interesting read. A very interesting thing this says is that\n A neural network cannot have a unique optima if it has atleast two distinct node values in the same layer. Amazing , isn\u0026rsquo;t it ?\n I guess I haven\u0026rsquo;t been mathematically rigorous at all and have taken many assumptions throughout. Maybe I will list them someday!\n","date":1475274166,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579777673,"objectID":"4ac8e536a5b933d9e21a2297af3e5517","permalink":"https://amartya18x.github.io/post/nn_nonconvex/","publishdate":"2016-10-01T03:52:46+05:30","relpermalink":"/post/nn_nonconvex/","section":"post","summary":"I don\u0026rsquo;t remember where I heard this arguement but it was a pretty interesting one and I couldn\u0026rsquo;t remember the entire reasoning and so as I began to think about it, I figured some of it out and I believe it is pretty reasonable. We have all argued that neural networks are difficult to optimize because they are non-convex(or non-concave) for that matter. What we don\u0026rsquo;t talk about so much is why is it that they are non-convex.","tags":["academic"],"title":"NonConvexity of neural networks","type":"post"},{"authors":null,"categories":null,"content":"I am going to , inspired by the course on optimization that I am doing this semester, talk a bit about strong convexity and strong smoothness and our very popular gradient descent works on them. So, before going right into the details let\u0026rsquo;s have a quick chat about convexity in general and we do have a few ways of going about it.\nI will go about with talking about two definitions of convex functions, the first one being general more than the second. This is not to say that there aren\u0026rsquo;t more general or equivalent definitions, which there are, but that I like these better and this is probably what you will use more in your life if you have to use one at all.\nConvex Function $$\\frac{f(y)+f(x)}{2} \\ge f(\\frac{x+y}{2}) $$ Note that this does not require the function to be differentiable. But, with differetiable functions, one can actually get an easier and more productive definition.\nConvex differetiable functions $$ f(y) \\ge f(x) + \\langle \\nabla f(x), y - x \\rangle $$\nSubgradients The interesting thing about this is that this can tolerate functions, which are not differntiable in a finite number of points. We need to define subgradients for that though.\n$$ g(x) = \\{ g(x) | \\langle g(x), x_0 - x\\rangle \\forall y \\} $$\nAnd hence let\u0026rsquo;s do the intuitive thing of replacing $\\nabla f(x)$ with $g(x)$. So, now we have\n$$ f(y) \\ge f(x) + \\langle g(x), y - x \\rangle $$\nStrongly Convex function Now that we know of convex functions mathematically, it is intuitively a function such that if we draw a tangent plane(line in case of one variable), the function lies above the tangent plane at all points. Mathematically, that would be\n$$ f(y) \\ge f(x) + \\langle g(x), y - x \\rangle + \\frac{\\alpha}{2} \\| y - x\\|^2 $$ and $\\alpha$ is known as the strong convexity parameter. One can also, for doubly differntiable functions, say $ \\nabla^2 f(x) \\succeq \\alpha I $, where $\\nabla^2 f(x)$ is the hessian matrix and $ I $ is the identity matrix.\nStrongly Smooth function A strongly smooth function is just the opposite of the strongly convex function i.e. a function which lies below the tangent plane at all points. Mathematically, that would be\n$$ f(y) \\le f(x) + \\langle g(x), y - x \\rangle + \\frac{\\beta}{2} \\| y - x\\|^2 $$ and $\\beta$ is known as the strong smoothness parameter. One can also, for doubly differntiable functions, say $ \\beta I \\succeq \\nabla^2 f(x) $.\nGradient descent for strongly convex functions We know for a strongly convex function $f(x)$, $$ f(y) \\ge f(x) + \\langle g(x), y - x \\rangle + \\frac{\\alpha}{2} \\| y - x\\|^2 $$ A very nice property of these functions is that, we can actually bound $\\|f( x^{*}) - f(x)\\|$ where $(x^{*})$ is the optimal point , with the norm of the gradient ($\\|\\nabla f(x)\\|$). Let, $$ z = \\underset{y}{argmin }\\hspace{5pt} \\{ f(x) + \\langle g(x), y - x \\rangle + \\frac{\\alpha}{2} \\| y - x\\|^2 \\} $$\nThe definition of strong convexity is applicable for all y.\n\\begin{align*} z \u0026amp;= x - \\frac{\\nabla f(x)}{\\alpha} \\\\\nf(y) \u0026amp;\\ge f(x) + \\langle g(x), y - x \\rangle + \\frac{\\alpha}{2} \\| y - x\\|^2 \\\\\n\u0026amp;\\ge f(x) + \\langle g(x), \\frac{\\nabla f(x)}{\\alpha} \\rangle + \\frac{\\alpha}{2} \\| \\frac{\\nabla f(x)}{\\alpha} \\|^2 \\\\\n\u0026amp;\\ge f(x) - \\frac{1}{2\\alpha} | \\nabla f(x)|^2 \\\\\n\\therefore f(x^{*}) \u0026amp;\\ge f(x) - \\frac{1}{2\\alpha} \\| \\nabla f(x)\\|^2 \\end{align*}\nThis interesting bound also gives us a good convergence criterion. Set $f(x) - f(x^{*}) = \\epsilon$ i.e. the desired error. We only need to do the gradient descent until the gradient has reached $2\\alpha \\epsilon$ from the previous inequality. $$ 2\\alpha \\epsilon \\le \\frac{1}{2\\alpha} \\| \\nabla f(x)\\|^2 $$\nLike the function value above, we can also get a bound on the distance of the current point from the optimal point in terms of gradient.\n\\begin{align*} f(x^{*}) \u0026amp;\\ge f(x) + \\langle g(x), x^{*} - x \\rangle + \\frac{\\alpha}{2} \\| x^{*} - x\\|^2 \\\\\n\u0026amp;\\ge f(x) - \\underbrace{( \\{ \\| g(x)\\|\\| x^{*} - x \\| - \\frac{\\alpha}{2} \\| x^{*} - x\\|^2 \\}) }_a \u0026amp;\u0026amp; \\text{(By Cauchy\u0026rsquo;s inequality)}\\\\\n\\end{align*} As we know that $f(x^{*})$ is optimal $ f(x^{*}) \\le f(x)$, a \\ge 0 must be true.\n$$ \\| g(x)\\|\\| x^{*} - x \\| \\ge \\frac{\\alpha}{2} \\| x^{*} - x\\|^2 $$ Or, $$\\frac{2}{\\alpha} \\| g(x)\\| \\ge \\| x^{*} - x\\| $$\nAnalyzing the gradient descent Define, $$ \\Phi_t = f(x^t) - f(x^{*}) $$\n the lyapunov function. Decrease in the value of the lyapunov function means that the function is nearing its optima. Let\u0026rsquo;s also define another distance. $$ D_t = \\| x^{*} - x_t \\|_2 $$ \\begin{align*} \\label{eq:phi} f(x^{*}) \u0026amp;\\ge f(x_t) + \\langle \\nabla f(x_t), x^{*} - x_t \\rangle + \\frac{\\alpha}{2} \\| x^{*} - x_t\\|^2 \\\\\nf(x^{*}) - f(x_t) \u0026amp;\\ge \\hspace{2pt} \\langle\\nabla f(x_t), x^{*} - x_t \\rangle + \\frac{\\alpha}{2} \\| x^{*} - x_t\\|^2 \\\\\n\\end{align*}  Hence, we have the following inequality with the lyapunov function. \\begin{equation} \\Phi_t \\le -\\langle g(x_t), x^{*} - x_t \\rangle - \\frac{\\alpha}{2} \\| x^{*} - x_t\\|^2 \\end{equation} Lets work with $D_t$ and then try to relate it with $\\Phi_t$. \\begin{align*} D^2_{t+1} \u0026amp;= \\|x^{t+1} - x^{*} \\|_2^2 \\\\\n\u0026amp;= \\| x^t - \\eta_t g_t - x^{*}\\|_2^2 \\\\\n\u0026amp; = \\|x^t - x^{*} \\|_2^2 + \\eta_t^2 \\|g_t(x_t)\\|^2 - 2 \\eta_t \\langle x^t - x^{*}, g_t\\rangle \\\\\nD^2_{t+1} - D^2_{t} \u0026amp;= \\eta_t^2 \\|g_t(x_t)\\|^2 - 2 \\eta_t \\langle x^t - x^{*}, g_t\\rangle \\\\\n\\langle x^t - x^{*}, g_t\\rangle \u0026amp;= \\frac{D^2_t - D^2_{t+1}}{2\\eta_t} + \\frac{\\eta_t \\|g_t\\|^2}{2} \\end{align*} Let\u0026rsquo;s plugin in this onto the previous definition of the lyapunov function. \\begin{align*} \\Phi_t \u0026amp;\\le -\\langle g(x_t), x^{*} - x_t \\rangle - \\frac{\\alpha}{2} \\| x^{*} - x_t\\|^2 \\\\\n\u0026amp;\\le \\frac{D^2_t - D^2_{t+1}}{2\\eta_t} + \\frac{\\eta_t \\|g_t\\|^2}{2} - \\frac{\\alpha}{2} \\| x^{*} - x_t\\|^2 \\\\\n\u0026amp;\\le \\frac{D^2_t - D^2_{t+1}}{2\\eta_t} + \\frac{\\eta_t \\|g_t\\|^2}{2} - \\frac{\\alpha}{2} D^2_t \\end{align*} It is difficult to show that $\\Phi_t$ is going to zero. It is considerably easier to use the sum for that. We will see a trick with jensen\u0026rsquo;s inequality. \\begin{align*} \\sum_{t=0}^T \\Phi_t \u0026amp;\\le \\sum_{t=0}^T (\\frac{D^2_t - D^2_{t+1}}{2\\eta_t} + \\frac{\\eta_t \\|g_t\\|^2}{2} - \\frac{\\alpha}{2} D^2_t) \\\\\n\u0026amp;\\le \\sum_{t=0}^T (\\frac{D^2_t}{2\\eta_t} + \\frac{\\eta_t \\|g_t\\|^2}{2} - \\frac{\\alpha}{2} D^2_t) - \\sum_{t=1} \\frac{D^2_{t}}{2\\eta_{t-1}} \u0026amp;\u0026amp; \\text{(A bit of change of variable)} \\\\\n\u0026amp;\\le D_0^2(\\frac{1}{2\\eta_0} - \\frac{\\alpha}{2}) + \\sum_{t=1}^T D^2_t(\\frac{1}{2\\eta_t} - \\frac{\\alpha}{2} + \\frac{1}{2\\eta_{t-1}}) + \\sum_{t=0}^T\\frac{\\eta_t G^2}{2} \u0026amp;\u0026amp;\\text{(Assuming bounded gradients again)} \\end{align*}\nNow, the first term is a constant and the third term is a sum of constants weighed by a parameter that we are fixing. So, the major problem is with the second term. Why note set it to zero ? We can do that by setting $\\eta\\_t = \\frac{1}{\\alpha t}$ \\begin{align*} \\sum_{t=0}^T \\Phi_t \u0026amp;\\le D_0^2(\\underbrace{\\frac{1}{2\\eta_0} - \\frac{\\alpha}{2}}_{\\le 0}) + \\sum_{t=1}^T D^2_t( \\underbrace{\\frac{1}{2\\eta_t} - \\frac{\\alpha}{2} + \\frac{1}{2\\eta_{t-1}}}_0) + \\sum_{t=0}^T\\frac{ G^2}{2\\alpha t} \u0026amp;\u0026amp;\\text{(Assuming bounded gradients again)} \\\\\n\\frac{1}{T}\\sum_{t=0}^T {\\Phi_t} \u0026amp;\\le \\frac{G^2 log(T)}{2T} \\\\\n\\sum_{t=0}^T \\frac{f(x^t) - f(x^{*})}{T} \u0026amp;\\le \\frac{G^2 log(T)}{2T} \\\\\n\\end{align*} Applying Jensen\u0026rsquo;s inequality because $\\Phi_t$ is convex $$f(\\frac{\\sum_{t=0}^T x^t}{T}) \\le \\sum_{t=0}^T \\frac{f(x^t) }{T} - f(x^{*}) \\le \\frac{G^2 log(T)}{2T} $$ To remove the $log(T)$, try doing a weighed sum of the $\\Phi_t$ with $t\\Phi_(t)$ and then don\u0026rsquo;t forget to divide by $\\sum_{t=1}^{T}t$. It will work out smooth\nGradient descent for strongly smooth functions We will work with similar Lyapunov function $\\Phi\\_t$ and $D\\_t$\nAs $f(x)$ is strong smooth, $$ f(x^{t+1}) \\le f(x^t) + \\langle \\nabla f(x^t), x^{t+1} - x^t \\rangle + \\frac{\\beta}{2} \\| x^{t+1} - x^t \\|^2 $$ and as $f(x)$ is convex $$ f(x^{*}) \\ge f(x) + \\langle \\nabla f(x), x^{*} - x \\rangle $$ By rearranging terms we get, $$ f(x) \\le f(x^{*}) + \\langle \\nabla f(x), x - x^{*} \\rangle $$ Plugging in this inequality into the strong smoothness inequality and replacing the $\\Phi_{t+1}$ and $D_t$ in the correct places gives us the following\n\\begin{align*} \\Phi_{t+1} \u0026amp;\\le \\langle \\nabla f(x), x^t - x^{*}\\rangle + \\langle \\nabla f(x^t), x^{t+1} - x^t \\rangle + \\frac{\\beta}{2} \\| x^{t+1} - x^t \\|^2 \\\\\n\u0026amp;\\le \\langle \\nabla f(x), x^t - x^{*} + x^{t+1} - x^t \\rangle + \\frac{\\beta}{2} \\| x^{t+1} - x^t \\|^2 \\\\\n\u0026amp;\\le \\langle \\nabla f(x), x^{t+1} - x^{*} \\rangle + \\frac{\\beta}{2} \\| x^{t+1} - x^t \\|^2 \\\\\n\\end{align*}\nNow, we know the following $x^{t+1} = x^t - \\eta\\_t \\nabla f(x^t)$ which gives us $\\nabla f(x^t) = \\frac{x^t - x^{t+1}}{\\eta\\_t} $\n\\begin{align*} \\Phi_{t+1} \u0026amp;\\le \\langle \\nabla f(x), x^{t+1} - x^{*} \\rangle + \\frac{\\beta}{2} \\| x^{t+1} - x^t \\|^2 \\\\\n\u0026amp;\\le \\frac{1}{\\eta_t}\\langle x^{t} - x^{t+1}, x^{t+1} - x^{*} \\rangle + \\frac{\\beta}{2} \\| x^{t+1} - x^t \\|^2 \\\\\n\\end{align*} Now, we need to evaluate the term $\\langle x^{t} - x^{t+1}, x^{t+1} - x^{*} \\rangle $ \\begin{align*} \\|x^t - x^{*}\\|^2 \u0026amp;= \\|x^t - x^{t+1} + x^{t+1} - x^{*}\\| \\\\\n\u0026amp;=\\|x^t - x^{t+1}\\|^2 + \\|x^{t+1} - x^{*}\\| + 2\\langle x^{t} - x^{t+1}, x^{t+1} - x^{*} \\rangle \\\\\n\\langle x^{t} - x^{t+1}, x^{t+1} - x^{*} \\rangle \u0026amp;= \\frac{1}{2}( D_{t}^2 - D_{t+1}^2 - \\|x^{t+1} - x^{*}\\| ) \\end{align*} Plugging this back into the inequality we have above, we get the following\n\\begin{align*} \\Phi_{t+1} \u0026amp;\\le \\frac{1}{2\\eta_t} (D_{t}^2 - D_{t+1}^2 - \\frac{1}{2}\\|x^{t+1} - x^{*}\\| )+ \\frac{\\beta}{2} \\| x^{t+1} - x^t \\|^2 \\\\\n\u0026amp;\\le \\frac{1}{2\\eta_t} (D_{t}^2 - D_{t+1}^2) - \\| x^{t+1} - x^t \\|^2 (\\frac{1}{2\\eta_t} - \\frac{\\beta}{2}) \\end{align*} Set $\\eta_t = \\frac{c}{\\beta} $ where, $c\\in [0,1]$ . We get($0 \\ge k\\le 1) $ $$ (\\frac{1}{2\\eta_t} - \\frac{\\beta}{2}) = \\frac{\\beta}{2} (\\frac{1}{k} - 1)\\ge 0 $$\n\\begin{align*} \\sum_{t=0}^T \\Phi_{t+1} \u0026amp;\\le \\sum_{t=0}^T \\frac{\\beta}{2c} (D_{t}^2 - D_{t+1}^2)\\\\\n\\frac{1}{T} \\sum_{t=0}^T \\Phi_{t+1} \u0026amp;\\le \\frac{\\beta}{2cT} (D_{0}^2 - D_{T+1}^2) \\end{align*} Applying Jensen\u0026rsquo;s inequality because $\\Phi_t$ is convex $$f(\\frac{\\sum_{t=0}^T x^t}{T}) \\le \\sum_{t=0}^T \\frac{f(x^t) }{T} - f(x^{*}) \\le \\frac{\\beta}{2cT} (D_{0}^2 ) $$\nFor both the strongly convex and the strong smooth functions, we have seen the average selector. Is it possible that some other selector can give us better bounds ? Selector refers to the particular way of choosing the $x$, which we want the function to return.\n","date":1473963580,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579777673,"objectID":"37cfa9d807bdcadfc217d318291c3b01","permalink":"https://amartya18x.github.io/post/strconvex/","publishdate":"2016-09-15T23:49:40+05:30","relpermalink":"/post/strconvex/","section":"post","summary":"I am going to , inspired by the course on optimization that I am doing this semester, talk a bit about strong convexity and strong smoothness and our very popular gradient descent works on them. So, before going right into the details let\u0026rsquo;s have a quick chat about convexity in general and we do have a few ways of going about it.\nI will go about with talking about two definitions of convex functions, the first one being general more than the second.","tags":["academic"],"title":"Strong Convexity and Strong Smoothness","type":"post"},{"authors":null,"categories":[],"content":"   Montreal from Mont Royal   As I said before, during the Summers of 2016, I went to Montreal to work as a Research Intern in MILA(Montreal Institute of Learning Algorithms) and there is no way I cannot admit, it was a very wonderful experience. Apart from being a very information-rich span of two and a half months, it was a very beautiful place to visit. In fact, it was too beautiful to not write a separate post about the non-technical experiences of my visit. Well, the process of acquiring the visa was not a very easy task. As a matter of fact, I got my visa in my hands a few hours before my departure, which was a few hours after my semesters ended. So, to understand the climate, it was a pretty tense and action packed couple of days. However, things went smoothly towards the end and I did get my visa and my flight in time and so, off I flew. It was a one hop flight to Montreal via Zurich and Zurich was probably even more beautiful than Montreal(atleast from the air and the terminal it was, where I had a 6 hrs stop). The airport seemed to be surrounded my mountains and forests and I landed there pretty early thus giving me a chance to catch a glimpse of the sunrise from behind the mountains. On my way to Zurich, I sat beside a swiss woman, who realizing I was on my first international travel kindly offered me the window seat. She was on her way from Bhutan, where her husband worked in an NGO, to Zurich. She read a book about the history of Bhutan throughout the journey. I slept. I woke up in time though to catch the sunrise.   Zurich Sunrise   In the next part of the journey, I was accompanied by a Quebecois boy, who was in Poland visiting his friends. I slept there too. Immigration through Montreal was pretty cool. However, when the Border Service Agent asked me my field of study, I had a difficult time trying to figure out what to say - Deep Learning, Machine Learning, Artificial Intelligence ? I said Computer Science though. She asked me if I write apps for iphone and I said yes. I also made the mistake of asking the agent if the swiss knife I had was a weapon, which made her frown quite a lot. I quickly explained that it was rusted and I never really used it other than cutting packets and it was not an original either. But the big problem was when she noticed I had ticked dried fruits in my immigration form. As my mother had packed them, I barely knew what was in it. So, I said it had cashew and some other stuff. And she asked , \u0026ldquo;What other stuff ?\u0026rdquo;. I said, \u0026ldquo;Buiscuits, raisins and some similar stuff.\u0026rdquo; She asked, \u0026ldquo;What similar stuff ?\u0026rdquo; I said, though I was sure I didnt have them, \u0026ldquo;Apricots probably.\u0026rdquo; And she asked, \u0026ldquo;Anything else?\u0026rdquo;. I said , \u0026ldquo;No! Just those.\u0026rdquo;. I still think she was expecting me to say, \u0026ldquo;Marijuana!!\u0026rdquo;. Why else would someone grill me regarding dried food in my bag. As I escaped immigration, the next part was getting to my apartment, which I had booked via Airbnb. I knew the address and so, I asked a police officer who asked me to take the bus and then a couple in the bus, who told me to get off the bus at Lionel Groulx(Mind you things were in French!) and take the metro and then an old man in the metro, who told me where to get off the metro and finally, I walked the rest of the way to my house. I must admit it felt pretty amazing that day to be in Montreal and I had a nice sleep(Inspite of sleeping all the way). The next day, I went to the local Metro(which is a grocery) and bought stuff, I thought I could manage with to satiate my intestines. It was an amazing walk and I noticed I stayed very close to the St. Joseph Oratoire. I didnt get a SIM card though. I thought it wasn\u0026rsquo;t quite worth it as internet plans(i.e. ones which offered substantial amounts of data) were priced quite high and local calling wasn\u0026rsquo;t very high on my demand list. Apparently, it didnt go as well for me.   St. Joseph Oratorie   Pretty soon, I visited my University and went to my department. People were away for a conference when I got there and so, the first week went lightly trying to read some papers, know some people, read some code and do this and that. Pretty soon, things started in full pace and I began to love the whole idea of venturing into Research but that\u0026rsquo;s a story for another day. A couple of weeks later,a few people from IAESTE(that\u0026rsquo;s the organisation that helped me with the visa) met up(me included) and we went around boating in a canal at Le Charlevoux and then we went to Vieux Port(Old Port). I was amazed at the way Montreal had transformed a decaying buisness estate into a place for tourist attraction. We spent some time there and then walked the streets of Old Port which was so similar to the pictures I had seen of streets in Rome. We then had dinner at a place, where I had pasta with spiced bacon. I dont like pasta. That day, I loved it. There have been a few more sharp turns in my culinary tastes. Beefs, pork, aubergines - I absolutely loved the preparations I tasted there. My houseowner was a pretty awesome person. He introduced me to one another thing, I had definitely missed in life before going there - Backgammon. Being, a person who is absolutely fascinated by board games, backgammon was a totally awesome gift to my hobby-list. I played with him with quite often, usually on sundays. He took me to a friend of his, who also loved the game and we had a game in his well-maintained porch with cans of Apple Cidre. Once cannot not like the taste of Apple Cidres.   Cobbled Streets   The most awesome part of the trip was when and a friend of mine, who was also interning in the same university decided to go on a rather spontaneous unplanned trip to a village.(It\u0026rsquo;s just a village, we had no idea which village.) All we wanted was to see the Canadian countryside and the Laurentians were quite close and it would have been a crime to miss them. So yes - a village in the Laurentians. it would have been similarly insane to not atleast spend a night there. Keeping these in mind, we decided to spend a night somewhere in the Laurentians and I searched for a place on Airbnb in the general vicinity of the Laurentians within a reasonable price and we stumbled a pretty house in the woods. We planned to get the morning metro to the Montmorency station and then a local bus to a village called St. Jerome, which also happened to be the starting place of a cycling path called Petit train du Nord(The small train of the North). It was initally a rail track, which they converted to a cycling path when the place went outdated. It had a beautiful terrain through the mountains finally ending in Mont Laurier. We wished to go though the tour, atleast partly, however it was too costly. So, back to our traevelling plans. It all went wrong when we woke up late and missed the first metro, which in turn made us miss the bus to St. Jerome and which in turn made us miss the bus to Val David. Oh! I really I havent told you where we were going. We were going to a small village that lies somewhere in the centre of Les Petit train du Nord called Val David.   Val David   The good thing is, this forced us to spend some time in St. Jerome. There is a pretty little stream cutting through the village, a peculiar church(that somehow looked like a huge bakery to me), a contemporary art museum and a lot of people on cycle. Apparently, everyone was riding a cycle. Abled people rode high cycles, stunt cycles, mountain bikes or racing bikes. Differently abled people rode battery powered wheel chairs and some of them were pretty cool. We had a lunch of hot dog and vanilla Ice cream. it was a task in itself trying to order them as people didn\u0026rsquo;t speak english in the place. It was mostly show-and-tell. We ended up buying tickets for a different bus and the bus was pretty luxurious. We sped through a road though the Laurentians. Laurentians is an old range and has hence eroded over time. It is a popular skiing place , not the right time though. There were patches of ski trails that were clearly visible though the woods. I have always loved woods and mountains more than oceans for the serenity and the colorful scenery it provides. Along with us on the highway, there were a lot of pickup trucks and most of them carried bicycles on their backs. These people were going to start cycling on the train from Mont Laurier(i.e. in the upper part of the Laurentians) and come down to St. Jerome. I will put that on my bucketlist for now. After a very short ride the bus dropped us off at the crossing of Val David. We were quite hungry and we noticed a Tim Hortons. Tim Hortons is the biggest fast food chain(takeaway food chain) in Canada and its quite better than the McDs there. I had some sandwich those guys made along with coffee and home fries. However, the girl at the counter forgot to add the fries and the coffee and charged us for the sandwich only. When I mentioned that I had asked for fries and coffee too, even after insisting repeatedly she didn\u0026rsquo;t take our money and gave them to us for free. Somehow, I hold free food-givers quite high in my list of respected people.   Starting point of Petit Train du Nord   Our house was some 5-6 km away i.e. inside Val David and there wasn\u0026rsquo;t any public transport available. Assisted by google maps and hoping for free rides, we started walking. It was 6 and still bright. Curious as we were, we took a diversion when we saw a lake on the map. We walked via the cycling trail till we reached a lake and it was such a pretty lake. Then we noticed a regional parc near it and just to be clear parcs mean some kind of forests and so, we decided to make it to the parc. On reaching the parc, we asked the guard if we could go through the parc. He told us to come back tomorrow as it would get pretty dark soon and it wouldn\u0026rsquo;t be safe. I remember him telling us there weren\u0026rsquo;t many bear parc as the parc wasn\u0026rsquo;t too big and we wondered , who would want to go hiking in a parc with bears and canadian bears are not the cute kind.   A house by the lake   So, we walked back all the way and started walking towrds the house, which was still 5-6 kms away. It was possibly one of the most beautiful roads, I have ever walked on. There were curious little bungalows on either sides of the road, not too dense but one every once in a while. Some of the houses were pretty cool.   A beautiful house   And on we went down the road for another couple of hours. Went by houses and lakes and a car once in a while too. A very sweet gesture, we noticed, of the people down there was that every one passing by us in a car would not at us. Nevertheless, amidst all these interesting nodding interactions, we reached the house, we were to stay in for the night. Dont go by the looks though. Old though it might look, it was pretty well furnished in the inside. When we reached, the hosts were out but we had the security key and so we let ourselves in, cleaned up and waited inside(a bit hesitant to use things) for the hosts to arrive.The house was backed by the woods and we slept that night staring out a huge window into a very dense wood. It was a bit scary and I am not ashamed to admit it.   House in the woods   Well, the next morning we walked all the way back to the Tim Hortons for breakfast(i.e. 2 and a half hours) and then walked to the parc and then began hiking. The parc had very human touch and it was dense forest with multiple trails crisscrossing through it. We went through one that led to a peak called Mt. Condor. We went to both the Nord(North) and Sud(South) quest of the peak. I had adopted a stick that day to help me deal with the big blood sucking flies/mosquitoes that appeared once in a while and appeared to have imprinted on me. I did read somewhere that this is because I have an attractive body odour. I doubt it though. It never works with any other being other than mosquitoes.   Me with my stick   To put things in a shell, I was highly impressed by the beauty and the serenity of the place and the friendliness of the place and also the fact that I scaled my first official peal(Only 440 metres though)   The trail   The next few weeks went pretty quietly and busy with work. We,(Shivsankar and me), went out almost every night to visit parts of the city. We would often take the bus and get down somewhere and start walking. One night, before we had the bus pass,miser that we were, we didnt want to spend on a bus ticket and we walk almost 16km around Mont Royal to walk to our home. It was 1 am by the team we reached. Other days, we went to a pub called Randolph, which was a board games pub and it was so beautiful and we had grown a friendly acquaintance with the gamemasters, bartender and waiters. Randolph had been mine most frequently visited place in Montreal. I often wondered during my visits to the pub, how wonderful a job it was to teach people how to play board games and then get paid for it.(Canada also has some minimum wage law I guess) We would also take the Metro to other places like Place des Arts and St. Catherine etc. Another person, without whom the trip wouldn\u0026rsquo;t have been as enjoyable as it was is Anirudh. We would often go out to eat(Not many times though). We went to a Jazz festival and I discovered I absolutely love Jazz music though it might have just been the concert effect. It was the grand show by Jamie Cullum and I loved the song When I get Famous. I also had another tasty pizza at an Italian Place, which I had previously visited with Shivsankar. I was totally amazed at the number of different kinds of pizza they had to offer and the number was around 70. Pizzas were quite different there than it is here. The crust is usually thinner and they have sie fillings beneath the cheese which forms a kind of layer and then they have some toppings on it. I dont understand why they want to eat it with fork. A couple of days, I went to a greek place, where I discovered another culinary sweetheart called Souvlaki in Tsaziki. I dont think I can do justice to that great dish by trying to describe it. So just google it and watch the imagies and drool. A few dys before I left, another guy from my lab Tanel Parnamaa finished his intern and went back to Estonia. So, we went out to see him off. We went to a Japanese Raman place and yes, it was awesome too. I believe I had octopus that day too. So, I tasted quite a lot different cuisines there including lebanese, portugese, french, peruvian, greek, thai, vietnamese, quebecoise and Italian. Maybe I will write a seperate post about the food there. It is just that I forget to take pictures before eating them. There was a place called Brulerie\u0026rsquo;s and Luc de Lorraine quite close to the University. One should absolutely visit them. Brulerie is a really cheap place for weekdays early breakfast. It gives you a choice of meat, a couple of eggs(in whatever style), bread, home fries , fruits and unlimited coffee for only 7$, which is cheap. Don\u0026rsquo;t convert it to INR. Just take my word for it that it is cheap. Luc de Lorraine is however pretty costly but serves absolutely delicious french lunch.   Dinner for Tanel with Dima, Anirudh, Krishna, Rosemary and Zander   I had my parting dinner in a vegan thai place with some pretty awesome people including Anirudh, Ishmael, Zander, Rosemary and Krishna.   My Dinner   I can\u0026rsquo;t explain how badly I miss the place. It is an absolutely must-go place for - everyone.   Will come back.   ","date":1469554154,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579777673,"objectID":"7271bb720a79d60a525e53c5e6f8b5ee","permalink":"https://amartya18x.github.io/post/montreal/","publishdate":"2016-07-26T18:29:14+01:00","relpermalink":"/post/montreal/","section":"post","summary":"Montreal from Mont Royal   As I said before, during the Summers of 2016, I went to Montreal to work as a Research Intern in MILA(Montreal Institute of Learning Algorithms) and there is no way I cannot admit, it was a very wonderful experience. Apart from being a very information-rich span of two and a half months, it was a very beautiful place to visit. In fact, it was too beautiful to not write a separate post about the non-technical experiences of my visit.","tags":["personal"],"title":"Montreal","type":"post"}]