[{"authors":["admin"],"categories":null,"content":"Biography I am Amartya Sanyal, currently a D.Phil(PhD) student at the Department of Computer Science at the University of Oxford working with Prof. Varun Kanade and Prof. Phil H.S. Torr. I completed by Bachelor Of Technology in Computer Science And Engineering from the Indian Institute of Technology, Kanpur in 2017.\nI am interested in understanding why Deep Neural Networks, in whatever form, perform as good as they do. My research revolves around understanding mathematical properties regarding the structures of these networks and designing and analysing optimization algorithms that can exploit their properties. In the process, I also hope to come up with some practical applications using these findings. I am also interested in Optimization and Learning Theory in general.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1579782233,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"amartya18x.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"amartya18x.github.io/authors/admin/","section":"authors","summary":"Biography I am Amartya Sanyal, currently a D.Phil(PhD) student at the Department of Computer Science at the University of Oxford working with Prof. Varun Kanade and Prof. Phil H.S. Torr. I completed by Bachelor Of Technology in Computer Science And Engineering from the Indian Institute of Technology, Kanpur in 2017.\nI am interested in understanding why Deep Neural Networks, in whatever form, perform as good as they do. My research revolves around understanding mathematical properties regarding the structures of these networks and designing and analysing optimization algorithms that can exploit their properties.","tags":null,"title":"Amartya Sanyal","type":"authors"},{"authors":["Amartya Sanyal","Philip H.S. Torr","Puneet K Dokania"],"categories":null,"content":"","date":1576713600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579782233,"objectID":"b8602d83a6549b38cd514d51c854fe30","permalink":"amartya18x.github.io/publication/stable/","publishdate":"2019-12-19T00:00:00Z","relpermalink":"amartya18x.github.io/publication/stable/","section":"publication","summary":"Exciting new work on generalization bounds for neural networks (NN) given by Bartlett et al. (2017); Neyshabur et al. (2018) closely depend on two parameter- dependant quantities: the Lipschitz constant upper bound and the stable rank (a softer version of rank). Even though these bounds typically have minimal practical utility, they facilitate questions on whether controlling such quantities together could improve the generalization behaviour of NNs in practice. To this end, we propose stable rank normalization (SRN), a novel, provably optimal, and computationally efficient weight-normalization scheme which minimizes the stable rank of a linear operator. Surprisingly we find that SRN, despite being non-convex, can be shown to have a unique optimal solution. We provide extensive analyses across a wide variety of NNs (DenseNet, WideResNet, ResNet, Alexnet, VGG), where applying SRN to their linear layers leads to improved classification accuracy, while simultaneously showing improvements in genealization, evaluated empirically usingâ€”(a) shattering experiments (Zhang et al., 2016); and (b) three measures of sample complexity by Bartlett et al. (2017), Neyshabur et al. (2018), \u0026 Wei \u0026 Ma. Additionally, we show that, when applied to the discriminator of GANs, it improves Inception, FID, and Neural divergence scores, while learning mappings with low empirical Lipschitz constant.","tags":null,"title":"Stable Rank Normalization for Improved Generalization in Neural Networks and GANs","type":"publication"},{"authors":["Amartya Sanyal","Matt kusner","Adria Gascon","Varun Kanade"],"categories":null,"content":"","date":1525996800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579782233,"objectID":"e83a85ba8e4e8a99b77242172592406c","permalink":"amartya18x.github.io/publication/epaas/","publishdate":"2018-05-11T00:00:00Z","relpermalink":"amartya18x.github.io/publication/epaas/","section":"publication","summary":"Machine learning methods are widely used for a variety of prediction problems. Prediction as a service is a paradigm in which service providers with technological expertise and computational resources may perform predictions for clients. However, data privacy severely restricts the applicability of such services, unless measures to keep client data private (even from the service provider) are designed. Equally important is to minimize the amount of computation and communication required between client and server. Fully homomorphic encryption offers a possible way out, whereby clients may encrypt their data, and on which the server may perform arithmetic computations. The main drawback of using fully homomorphic encryption is the amount of time required to evaluate large machine learning models on encrypted data. We combine ideas from the machine learning literature, particularly work on binarization and sparsification of neural networks, together with algorithmic tools to speed-up and parallelize computation using encrypted data.","tags":null,"title":"TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service","type":"publication"},{"authors":["Amartya Sanyal","Varun Kanade","Philip H.S. Torr"],"categories":null,"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579782233,"objectID":"01fd45ce39bee703e80ca5749009ef66","permalink":"amartya18x.github.io/publication/lr_layer/","publishdate":"2018-04-01T00:00:00Z","relpermalink":"amartya18x.github.io/publication/lr_layer/","section":"publication","summary":"A key feature of neural networks, particularly deep convolutional neural networks, is their ability to 'learn' useful representations from data. The very last layer of a neural network is then simply a linear model trained on these 'learned' representations. Despite their numerous applications in other tasks such as classification, retrieval, clustering etc., a.k.a. transfer learning, not much work has been published that investigates the structure of these representations or whether structure can be imposed on them during the training process. In this paper, we study the dimensionality of the learned representations by models that have proved highly succesful for image classification. We focus on ResNet-18, ResNet-50 and VGG-19 and observe that when trained on CIFAR10 or CIFAR100 datasets, the learned representations exhibit a fairly low rank structure. We propose a modification to the training procedure, which further encourages low rank representations of activations at various stages in the neural network. Empirically, we show that this has implications for compression and robustness to adversarial examples.","tags":null,"title":"Learning Low Rank Representations","type":"publication"},{"authors":["Amartya Sanyal","Purushottam Kar","Pawan Kumar","Sanjay Chawla"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579782233,"objectID":"7e4a081efbce15b3b73a30bcdc6210eb","permalink":"amartya18x.github.io/publication/non_dec/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"amartya18x.github.io/publication/non_dec/","section":"publication","summary":"We present a class of algorithms capable of directly training deep neural networks with respect to task-specific performance measures such as the F-measure and the Kullback-Leibler diver- gence that are structured and non-decomposable. This presents a departure from standard deep learning techniques that typically use squared or cross-entropy loss functions, that are decompos- able, to train the networks.","tags":null,"title":"Optimizing Non-decomposable Measures with Deep Networks","type":"publication"},{"authors":null,"categories":null,"content":"I don't remember where I heard this arguement but it was a pretty interesting one and I couldn't remember the entire reasoning and so as I began to think about it, I figured some of it out and I believe it is pretty reasonable. We have all argued that neural networks are difficult to optimize because they are non-convex(or non-concave) for that matter. What we don't talk about so much is why is it that they are non-convex. It probably has simpler answers where you will argue something about the non-linearities involved. This post has to do with a much more interesting answer.\nTo understand the reasoning, we have to argue that if the function has atleast two local minimas such that their mid-point is not a local minima, the function is non-convex. In formal terms, $$\\text{If }\\exists x,y \\in Dom(f) \\hspace{5pt }s.t.\\hspace{5pt} g(x) = g(y) = 0 $$ and $$\\forall z\\in [x,y] \\text{ s.t. } g(z) \\neq 0 $$ where $g(x)$ is a subgradient at $x$ the function is non-convex.\nThis is pretty simple to follow from the Mid-point value theorem and the definition of convexity.\nConsider a neural network $F(\\cdot)$ with a few layers and name three layers $L_1, L_2$ and $ L_3$ Consider nodes $a$ and $b$ in $L_1$ and $c$ and $d$ in $L_2$.Let the parameters connecting node $i$ to $j$ be called $w_{ij}$.\nThee arguement to the function can be thought of as $A=[\\cdots w_{ac}, w_{ad}, \\cdots , w_{bc}, w_{bd}\\cdots]$ and the function evaluation as $f(A)$. Consider $B=[\\cdots w_{ad}, w_{ac},\\cdots, w_{bd}, w_{bc}, \\cdots]$. I can claim that there exists a $B$ i.e. where $ w_{ad}, w_{ac}$ and $ w_{bd}, w_{bc} $ are swapped respectively, there exists an ordering of the other weights(permute the other edges) such that the function evaluation remains same. It is pretty easy to see how it can be true.\n Look at the following picture where the reg edges come into one node and the green into another.\n     Before permutation!   For reference consider\n The first red edge coming out of the top left node as $w_{ac}$ The gree edge out of the same node as $w_{ad}$. The red edge out of the second node in the same layer as $w_{bc}$ The green edge out of the second node as $ w_{bd} $.  Connect all edges coming in to $L_2$ from $L_1$ into node $d$ to node $c$ and vice versa and then shift the origin of all outgoing edges from node $c$ to $L_3$ to node $d$ and vice versa. What you have acheived is the network that you would have achieved by holding the two nodes by your hand and manually exchanging their positions while keeping the connecting edges intact. And Voila! now you have $A, B \\in dom(F)$ where $F(A) = F(B) $.\n Now look at them after the edges are shifted.    After permutation!    The natural doubt you might suddenly have is What about a linear network ? . Well, it is a well known fact that such a function can be stated as $F(x; A, b) = Ax + b$ for some matrix $A$ and vector $B$. And as you know, linear layers do not have a maxima or a minima and hence the whole assumption we made about having $g(x) = 0$ i.e. zero subgradients, do not hold. Thus, it does not contradict the statement given earlier.\nWe will also need to understand how it holds the other point about there existing some point between the local optimas such that it does not have the same value as the optima. To understand this lets look at simplified representation of $F(\\cdot)$. Instead, of taking the weights of the edges to be the arguements of $F(\\cdot)$, let the nodes be the arguements.\n$$F(n_{1,1}, n_{1, 2}, \\cdots, n_{3,1}\\cdots)$$ where $n_{i, j}$ is the $j^{th}$ node in $i^{th}$ layer. $$n_{i,j} = \\sum_{j = 1}^k w_{j,k}^{i-1}\\sigma (n_{i-1, k})$$ where $w_{j,k}^{i-1}$ is the weight connecting the $k^{th}$ node in ${i-1}^{th}$ layer to the $j^{th}$ node in layer $i$.\nNote that the transformation from the earlier definition to this definition is many-to-one and not one-to-one, which is easy to guess because of the reduction in the number of parameters.\nNow, it is also easy to observe that if we permute $n_{1,1}$ amd $n_{1,2}$, $F(\\cdot)$ doesn't change it's value.To see how this permuation relates to the original neural network, we barely need to change the weights in the original function as mentioned above(In the diagram interchange the two nodes in the second layer) i.e. the permutation of the weights mentioned above is equaivalent to this permutation.\n What we can infer from this is that for each unique optimal value, there exists $\\prod_{i=1}^k n_k!$ points in the parameter space that can achieve that local optima, where $n_k$ is the number of nodes in the $k^{th}$ layer. Turns out that the number of saddle points are exponentially more!\n The above statement however holds true only when the values of the $n_k$ nodes in that layer are also distinct but I guess the idea is clear and it is more of discrete mathematics.\n What this might look like is shown below. Note that this is not the loss function of a neural network but rather a function called Rastrigin function taken from wikipedia.\n    Many many optimas!   To observe that the points between the local minimas are suboptimal, you will probably have to figure something out more rigorous as I do not have a very tight arguement, but lets look at a very interesting intuition.\nLet's assume that the function is convex. This would allow the function to be hit by jensen's inequaltiy which states that for a convex function $g(\\cdot)$ $$g(\\frac{1}{k}\\sum_{i = 1}^k x_i) \\le \\frac{1}{k} \\sum_i g(x_i)$$ We know that the nodes are permutable within a layer. Let there be $M$ layers and $N_i$ nodes in each layer. We can thus get $$\\prod_{i=1}^M N_i !$$ total permutations(Doesn't really matter if they are unique or not). If you apply jensen's inequality to this, what we will have is that the function evaluation at the average is less than or equal to the average of these optimal points, which are all equal and hence what we get is that the evaluation at the average point is also optimal(If it is not, the function is not convex).\nBut notice that we are actually averaging all the permutations possible in a layer and thus what we will get is a set of arguements such that the nodes that belong to the same layer have the same value. To understand this look at the matrices below and assume one layer corresponds to a column. The two matrices are thus independant permutations of the two columns.[ M_1= \\begin{bmatrix} 1 \u0026amp; 2 \\\\\\\n3 \u0026amp; 4 \\end{bmatrix} ]\n[ M_2= \\begin{bmatrix} 3 \u0026amp; 4 \\\\\\\n1 \u0026amp; 2 \\end{bmatrix} ]\n[ \\frac{M_1 + M_2}{2}= \\begin{bmatrix} 2 \u0026amp; 3 \\\\\\\n2 \u0026amp; 3 \\end{bmatrix} ] What this effectively means is that if the neural network was indeed convex, there would be a value which could be given to all nodes in a layer and yet the network would have represented a local optima. This is higly absurd and is very apparent if you consider a classification network where the final outputs should not have the same value in all nodes given some value in the input layer(which is also a part of the function arguement).\nTo generalize this to any neural network, you will have to look at the activation functions and their properties. The claim you will be trying to verify is that the average of two permutations of an optimal point need not be optimal.\nI guess this was an interesting read. A very interesting thing this says is that\n A neural network cannot have a unique optima if it has atleast two distinct node values in the same layer. Amazing , isn't it ?\n I guess I haven't been mathematically rigorous at all and have taken many assumptions throughout. Maybe I will list them someday!\n","date":1475274166,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579777673,"objectID":"4ac8e536a5b933d9e21a2297af3e5517","permalink":"amartya18x.github.io/post/nn_nonconvex/","publishdate":"2016-10-01T03:52:46+05:30","relpermalink":"amartya18x.github.io/post/nn_nonconvex/","section":"post","summary":"I don't remember where I heard this arguement but it was a pretty interesting one and I couldn't remember the entire reasoning and so as I began to think about it, I figured some of it out and I believe it is pretty reasonable. We have all argued that neural networks are difficult to optimize because they are non-convex(or non-concave) for that matter. What we don't talk about so much is why is it that they are non-convex.","tags":["academic"],"title":"NonConvexity of neural networks","type":"post"},{"authors":null,"categories":null,"content":"I am going to , inspired by the course on optimization that I am doing this semester, talk a bit about strong convexity and strong smoothness and our very popular gradient descent works on them. So, before going right into the details let's have a quick chat about convexity in general and we do have a few ways of going about it.\nI will go about with talking about two definitions of convex functions, the first one being general more than the second. This is not to say that there aren't more general or equivalent definitions, which there are, but that I like these better and this is probably what you will use more in your life if you have to use one at all.\nConvex Function $$\\frac{f(y)+f(x)}{2} \\ge f(\\frac{x+y}{2}) $$ Note that this does not require the function to be differentiable. But, with differetiable functions, one can actually get an easier and more productive definition.\nConvex differetiable functions $$ f(y) \\ge f(x) + \\langle \\nabla f(x), y - x \\rangle $$\nSubgradients The interesting thing about this is that this can tolerate functions, which are not differntiable in a finite number of points. We need to define subgradients for that though.\n$$ g(x) = \\{ g(x) | \\langle g(x), x_0 - x\\rangle \\forall y \\} $$\nAnd hence let's do the intuitive thing of replacing $\\nabla f(x)$ with $g(x)$. So, now we have\n$$ f(y) \\ge f(x) + \\langle g(x), y - x \\rangle $$\nStrongly Convex function Now that we know of convex functions mathematically, it is intuitively a function such that if we draw a tangent plane(line in case of one variable), the function lies above the tangent plane at all points. Mathematically, that would be\n$$ f(y) \\ge f(x) + \\langle g(x), y - x \\rangle + \\frac{\\alpha}{2} \\| y - x\\|^2 $$ and $\\alpha$ is known as the strong convexity parameter. One can also, for doubly differntiable functions, say $ \\nabla^2 f(x) \\succeq \\alpha I $, where $\\nabla^2 f(x)$ is the hessian matrix and $ I $ is the identity matrix.\nStrongly Smooth function A strongly smooth function is just the opposite of the strongly convex function i.e. a function which lies below the tangent plane at all points. Mathematically, that would be\n$$ f(y) \\le f(x) + \\langle g(x), y - x \\rangle + \\frac{\\beta}{2} \\| y - x\\|^2 $$ and $\\beta$ is known as the strong smoothness parameter. One can also, for doubly differntiable functions, say $ \\beta I \\succeq \\nabla^2 f(x) $.\nGradient descent for strongly convex functions We know for a strongly convex function $f(x)$, $$ f(y) \\ge f(x) + \\langle g(x), y - x \\rangle + \\frac{\\alpha}{2} \\| y - x\\|^2 $$ A very nice property of these functions is that, we can actually bound $\\|f( x^{*}) - f(x)\\|$ where $(x^{*})$ is the optimal point , with the norm of the gradient ($\\|\\nabla f(x)\\|$). Let, $$ z = \\underset{y}{argmin }\\hspace{5pt} \\{ f(x) + \\langle g(x), y - x \\rangle + \\frac{\\alpha}{2} \\| y - x\\|^2 \\} $$\nThe definition of strong convexity is applicable for all y.\n\\begin{align*} z \u0026amp;= x - \\frac{\\nabla f(x)}{\\alpha} \\\\\nf(y) \u0026amp;\\ge f(x) + \\langle g(x), y - x \\rangle + \\frac{\\alpha}{2} \\| y - x\\|^2 \\\\\n\u0026amp;\\ge f(x) + \\langle g(x), \\frac{\\nabla f(x)}{\\alpha} \\rangle + \\frac{\\alpha}{2} \\| \\frac{\\nabla f(x)}{\\alpha} \\|^2 \\\\\n\u0026amp;\\ge f(x) - \\frac{1}{2\\alpha} | \\nabla f(x)|^2 \\\\\n\\therefore f(x^{*}) \u0026amp;\\ge f(x) - \\frac{1}{2\\alpha} \\| \\nabla f(x)\\|^2 \\end{align*}\nThis interesting bound also gives us a good convergence criterion. Set $f(x) - f(x^{*}) = \\epsilon$ i.e. the desired error. We only need to do the gradient descent until the gradient has reached $2\\alpha \\epsilon$ from the previous inequality. $$ 2\\alpha \\epsilon \\le \\frac{1}{2\\alpha} \\| \\nabla f(x)\\|^2 $$\nLike the function value above, we can also get a bound on the distance of the current point from the optimal point in terms of gradient.\n\\begin{align*} f(x^{*}) \u0026amp;\\ge f(x) + \\langle g(x), x^{*} - x \\rangle + \\frac{\\alpha}{2} \\| x^{*} - x\\|^2 \\\\\n\u0026amp;\\ge f(x) - \\underbrace{( \\{ \\| g(x)\\|\\| x^{*} - x \\| - \\frac{\\alpha}{2} \\| x^{*} - x\\|^2 \\}) }_a \u0026amp;\u0026amp; \\text{(By Cauchy's inequality)}\\\\\n\\end{align*} As we know that $f(x^{*})$ is optimal $ f(x^{*}) \\le f(x)$, a \\ge 0 must be true.\n$$ \\| g(x)\\|\\| x^{*} - x \\| \\ge \\frac{\\alpha}{2} \\| x^{*} - x\\|^2 $$ Or, $$\\frac{2}{\\alpha} \\| g(x)\\| \\ge \\| x^{*} - x\\| $$\nAnalyzing the gradient descent Define, $$ \\Phi_t = f(x^t) - f(x^{*}) $$\n the lyapunov function. Decrease in the value of the lyapunov function means that the function is nearing its optima. Let's also define another distance. $$ D_t = \\| x^{*} - x_t \\|_2 $$ \\begin{align*} \\label{eq:phi} f(x^{*}) \u0026amp;\\ge f(x_t) + \\langle \\nabla f(x_t), x^{*} - x_t \\rangle + \\frac{\\alpha}{2} \\| x^{*} - x_t\\|^2 \\\\\nf(x^{*}) - f(x_t) \u0026amp;\\ge \\hspace{2pt} \\langle\\nabla f(x_t), x^{*} - x_t \\rangle + \\frac{\\alpha}{2} \\| x^{*} - x_t\\|^2 \\\\\n\\end{align*}  Hence, we have the following inequality with the lyapunov function. \\begin{equation} \\Phi_t \\le -\\langle g(x_t), x^{*} - x_t \\rangle - \\frac{\\alpha}{2} \\| x^{*} - x_t\\|^2 \\end{equation} Lets work with $D_t$ and then try to relate it with $\\Phi_t$. \\begin{align*} D^2_{t+1} \u0026amp;= \\|x^{t+1} - x^{*} \\|_2^2 \\\\\n\u0026amp;= \\| x^t - \\eta_t g_t - x^{*}\\|_2^2 \\\\\n\u0026amp; = \\|x^t - x^{*} \\|_2^2 + \\eta_t^2 \\|g_t(x_t)\\|^2 - 2 \\eta_t \\langle x^t - x^{*}, g_t\\rangle \\\\\nD^2_{t+1} - D^2_{t} \u0026amp;= \\eta_t^2 \\|g_t(x_t)\\|^2 - 2 \\eta_t \\langle x^t - x^{*}, g_t\\rangle \\\\\n\\langle x^t - x^{*}, g_t\\rangle \u0026amp;= \\frac{D^2_t - D^2_{t+1}}{2\\eta_t} + \\frac{\\eta_t \\|g_t\\|^2}{2} \\end{align*} Let's plugin in this onto the previous definition of the lyapunov function. \\begin{align*} \\Phi_t \u0026amp;\\le -\\langle g(x_t), x^{*} - x_t \\rangle - \\frac{\\alpha}{2} \\| x^{*} - x_t\\|^2 \\\\\n\u0026amp;\\le \\frac{D^2_t - D^2_{t+1}}{2\\eta_t} + \\frac{\\eta_t \\|g_t\\|^2}{2} - \\frac{\\alpha}{2} \\| x^{*} - x_t\\|^2 \\\\\n\u0026amp;\\le \\frac{D^2_t - D^2_{t+1}}{2\\eta_t} + \\frac{\\eta_t \\|g_t\\|^2}{2} - \\frac{\\alpha}{2} D^2_t \\end{align*} It is difficult to show that $\\Phi_t$ is going to zero. It is considerably easier to use the sum for that. We will see a trick with jensen's inequality. \\begin{align*} \\sum_{t=0}^T \\Phi_t \u0026amp;\\le \\sum_{t=0}^T (\\frac{D^2_t - D^2_{t+1}}{2\\eta_t} + \\frac{\\eta_t \\|g_t\\|^2}{2} - \\frac{\\alpha}{2} D^2_t) \\\\\n\u0026amp;\\le \\sum_{t=0}^T (\\frac{D^2_t}{2\\eta_t} + \\frac{\\eta_t \\|g_t\\|^2}{2} - \\frac{\\alpha}{2} D^2_t) - \\sum_{t=1} \\frac{D^2_{t}}{2\\eta_{t-1}} \u0026amp;\u0026amp; \\text{(A bit of change of variable)} \\\\\n\u0026amp;\\le D_0^2(\\frac{1}{2\\eta_0} - \\frac{\\alpha}{2}) + \\sum_{t=1}^T D^2_t(\\frac{1}{2\\eta_t} - \\frac{\\alpha}{2} + \\frac{1}{2\\eta_{t-1}}) + \\sum_{t=0}^T\\frac{\\eta_t G^2}{2} \u0026amp;\u0026amp;\\text{(Assuming bounded gradients again)} \\end{align*}\nNow, the first term is a constant and the third term is a sum of constants weighed by a parameter that we are fixing. So, the major problem is with the second term. Why note set it to zero ? We can do that by setting $\\eta\\_t = \\frac{1}{\\alpha t}$ \\begin{align*} \\sum_{t=0}^T \\Phi_t \u0026amp;\\le D_0^2(\\underbrace{\\frac{1}{2\\eta_0} - \\frac{\\alpha}{2}}_{\\le 0}) + \\sum_{t=1}^T D^2_t( \\underbrace{\\frac{1}{2\\eta_t} - \\frac{\\alpha}{2} + \\frac{1}{2\\eta_{t-1}}}_0) + \\sum_{t=0}^T\\frac{ G^2}{2\\alpha t} \u0026amp;\u0026amp;\\text{(Assuming bounded gradients again)} \\\\\n\\frac{1}{T}\\sum_{t=0}^T {\\Phi_t} \u0026amp;\\le \\frac{G^2 log(T)}{2T} \\\\\n\\sum_{t=0}^T \\frac{f(x^t) - f(x^{*})}{T} \u0026amp;\\le \\frac{G^2 log(T)}{2T} \\\\\n\\end{align*} Applying Jensen's inequality because $\\Phi_t$ is convex $$f(\\frac{\\sum_{t=0}^T x^t}{T}) \\le \\sum_{t=0}^T \\frac{f(x^t) }{T} - f(x^{*}) \\le \\frac{G^2 log(T)}{2T} $$ To remove the $log(T)$, try doing a weighed sum of the $\\Phi_t$ with $t\\Phi_(t)$ and then don't forget to divide by $\\sum_{t=1}^{T}t$. It will work out smooth\nGradient descent for strongly smooth functions We will work with similar Lyapunov function $\\Phi\\_t$ and $D\\_t$\nAs $f(x)$ is strong smooth, $$ f(x^{t+1}) \\le f(x^t) + \\langle \\nabla f(x^t), x^{t+1} - x^t \\rangle + \\frac{\\beta}{2} \\| x^{t+1} - x^t \\|^2 $$ and as $f(x)$ is convex $$ f(x^{*}) \\ge f(x) + \\langle \\nabla f(x), x^{*} - x \\rangle $$ By rearranging terms we get, $$ f(x) \\le f(x^{*}) + \\langle \\nabla f(x), x - x^{*} \\rangle $$ Plugging in this inequality into the strong smoothness inequality and replacing the $\\Phi_{t+1}$ and $D_t$ in the correct places gives us the following\n\\begin{align*} \\Phi_{t+1} \u0026amp;\\le \\langle \\nabla f(x), x^t - x^{*}\\rangle + \\langle \\nabla f(x^t), x^{t+1} - x^t \\rangle + \\frac{\\beta}{2} \\| x^{t+1} - x^t \\|^2 \\\\\n\u0026amp;\\le \\langle \\nabla f(x), x^t - x^{*} + x^{t+1} - x^t \\rangle + \\frac{\\beta}{2} \\| x^{t+1} - x^t \\|^2 \\\\\n\u0026amp;\\le \\langle \\nabla f(x), x^{t+1} - x^{*} \\rangle + \\frac{\\beta}{2} \\| x^{t+1} - x^t \\|^2 \\\\\n\\end{align*}\nNow, we know the following $x^{t+1} = x^t - \\eta\\_t \\nabla f(x^t)$ which gives us $\\nabla f(x^t) = \\frac{x^t - x^{t+1}}{\\eta\\_t} $\n\\begin{align*} \\Phi_{t+1} \u0026amp;\\le \\langle \\nabla f(x), x^{t+1} - x^{*} \\rangle + \\frac{\\beta}{2} \\| x^{t+1} - x^t \\|^2 \\\\\n\u0026amp;\\le \\frac{1}{\\eta_t}\\langle x^{t} - x^{t+1}, x^{t+1} - x^{*} \\rangle + \\frac{\\beta}{2} \\| x^{t+1} - x^t \\|^2 \\\\\n\\end{align*} Now, we need to evaluate the term $\\langle x^{t} - x^{t+1}, x^{t+1} - x^{*} \\rangle $ \\begin{align*} \\|x^t - x^{*}\\|^2 \u0026amp;= \\|x^t - x^{t+1} + x^{t+1} - x^{*}\\| \\\\\n\u0026amp;=\\|x^t - x^{t+1}\\|^2 + \\|x^{t+1} - x^{*}\\| + 2\\langle x^{t} - x^{t+1}, x^{t+1} - x^{*} \\rangle \\\\\n\\langle x^{t} - x^{t+1}, x^{t+1} - x^{*} \\rangle \u0026amp;= \\frac{1}{2}( D_{t}^2 - D_{t+1}^2 - \\|x^{t+1} - x^{*}\\| ) \\end{align*} Plugging this back into the inequality we have above, we get the following\n\\begin{align*} \\Phi_{t+1} \u0026amp;\\le \\frac{1}{2\\eta_t} (D_{t}^2 - D_{t+1}^2 - \\frac{1}{2}\\|x^{t+1} - x^{*}\\| )+ \\frac{\\beta}{2} \\| x^{t+1} - x^t \\|^2 \\\\\n\u0026amp;\\le \\frac{1}{2\\eta_t} (D_{t}^2 - D_{t+1}^2) - \\| x^{t+1} - x^t \\|^2 (\\frac{1}{2\\eta_t} - \\frac{\\beta}{2}) \\end{align*} Set $\\eta_t = \\frac{c}{\\beta} $ where, $c\\in [0,1]$ . We get($0 \\ge k\\le 1) $ $$ (\\frac{1}{2\\eta_t} - \\frac{\\beta}{2}) = \\frac{\\beta}{2} (\\frac{1}{k} - 1)\\ge 0 $$\n\\begin{align*} \\sum_{t=0}^T \\Phi_{t+1} \u0026amp;\\le \\sum_{t=0}^T \\frac{\\beta}{2c} (D_{t}^2 - D_{t+1}^2)\\\\\n\\frac{1}{T} \\sum_{t=0}^T \\Phi_{t+1} \u0026amp;\\le \\frac{\\beta}{2cT} (D_{0}^2 - D_{T+1}^2) \\end{align*} Applying Jensen's inequality because $\\Phi_t$ is convex $$f(\\frac{\\sum_{t=0}^T x^t}{T}) \\le \\sum_{t=0}^T \\frac{f(x^t) }{T} - f(x^{*}) \\le \\frac{\\beta}{2cT} (D_{0}^2 ) $$\nFor both the strongly convex and the strong smooth functions, we have seen the average selector. Is it possible that some other selector can give us better bounds ? Selector refers to the particular way of choosing the $x$, which we want the function to return.\n","date":1473963580,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579777673,"objectID":"37cfa9d807bdcadfc217d318291c3b01","permalink":"amartya18x.github.io/post/strconvex/","publishdate":"2016-09-15T23:49:40+05:30","relpermalink":"amartya18x.github.io/post/strconvex/","section":"post","summary":"I am going to , inspired by the course on optimization that I am doing this semester, talk a bit about strong convexity and strong smoothness and our very popular gradient descent works on them. So, before going right into the details let's have a quick chat about convexity in general and we do have a few ways of going about it.\nI will go about with talking about two definitions of convex functions, the first one being general more than the second.","tags":["academic"],"title":"Strong Convexity and Strong Smoothness","type":"post"},{"authors":null,"categories":[],"content":"    Montreal from Mont Royal   As I said before, during the Summers of 2016, I went to Montreal to work as a Research Intern in MILA(Montreal Institute of Learning Algorithms) and there is no way I cannot admit, it was a very wonderful experience. Apart from being a very information-rich span of two and a half months, it was a very beautiful place to visit. In fact, it was too beautiful to not write a separate post about the non-technical experiences of my visit. Well, the process of acquiring the visa was not a very easy task. As a matter of fact, I got my visa in my hands a few hours before my departure, which was a few hours after my semesters ended. So, to understand the climate, it was a pretty tense and action packed couple of days. However, things went smoothly towards the end and I did get my visa and my flight in time and so, off I flew. It was a one hop flight to Montreal via Zurich and Zurich was probably even more beautiful than Montreal(atleast from the air and the terminal it was, where I had a 6 hrs stop). The airport seemed to be surrounded my mountains and forests and I landed there pretty early thus giving me a chance to catch a glimpse of the sunrise from behind the mountains. On my way to Zurich, I sat beside a swiss woman, who realizing I was on my first international travel kindly offered me the window seat. She was on her way from Bhutan, where her husband worked in an NGO, to Zurich. She read a book about the history of Bhutan throughout the journey. I slept. I woke up in time though to catch the sunrise.    Zurich Sunrise   In the next part of the journey, I was accompanied by a Quebecois boy, who was in Poland visiting his friends. I slept there too. Immigration through Montreal was pretty cool. However, when the Border Service Agent asked me my field of study, I had a difficult time trying to figure out what to say - Deep Learning, Machine Learning, Artificial Intelligence ? I said Computer Science though. She asked me if I write apps for iphone and I said yes. I also made the mistake of asking the agent if the swiss knife I had was a weapon, which made her frown quite a lot. I quickly explained that it was rusted and I never really used it other than cutting packets and it was not an original either. But the big problem was when she noticed I had ticked dried fruits in my immigration form. As my mother had packed them, I barely knew what was in it. So, I said it had cashew and some other stuff. And she asked , \u0026ldquo;What other stuff ?\u0026quot;. I said, \u0026ldquo;Buiscuits, raisins and some similar stuff.\u0026rdquo; She asked, \u0026ldquo;What similar stuff ?\u0026rdquo; I said, though I was sure I didnt have them, \u0026ldquo;Apricots probably.\u0026rdquo; And she asked, \u0026ldquo;Anything else?\u0026quot;. I said , \u0026ldquo;No! Just those.\u0026quot;. I still think she was expecting me to say, \u0026ldquo;Marijuana!!\u0026quot;. Why else would someone grill me regarding dried food in my bag. As I escaped immigration, the next part was getting to my apartment, which I had booked via Airbnb. I knew the address and so, I asked a police officer who asked me to take the bus and then a couple in the bus, who told me to get off the bus at Lionel Groulx(Mind you things were in French!) and take the metro and then an old man in the metro, who told me where to get off the metro and finally, I walked the rest of the way to my house. I must admit it felt pretty amazing that day to be in Montreal and I had a nice sleep(Inspite of sleeping all the way). The next day, I went to the local Metro(which is a grocery) and bought stuff, I thought I could manage with to satiate my intestines. It was an amazing walk and I noticed I stayed very close to the St. Joseph Oratoire. I didnt get a SIM card though. I thought it wasn't quite worth it as internet plans(i.e. ones which offered substantial amounts of data) were priced quite high and local calling wasn't very high on my demand list. Apparently, it didnt go as well for me.    St. Joseph Oratorie   Pretty soon, I visited my University and went to my department. People were away for a conference when I got there and so, the first week went lightly trying to read some papers, know some people, read some code and do this and that. Pretty soon, things started in full pace and I began to love the whole idea of venturing into Research but that's a story for another day. A couple of weeks later,a few people from IAESTE(that's the organisation that helped me with the visa) met up(me included) and we went around boating in a canal at Le Charlevoux and then we went to Vieux Port(Old Port). I was amazed at the way Montreal had transformed a decaying buisness estate into a place for tourist attraction. We spent some time there and then walked the streets of Old Port which was so similar to the pictures I had seen of streets in Rome. We then had dinner at a place, where I had pasta with spiced bacon. I dont like pasta. That day, I loved it. There have been a few more sharp turns in my culinary tastes. Beefs, pork, aubergines - I absolutely loved the preparations I tasted there. My houseowner was a pretty awesome person. He introduced me to one another thing, I had definitely missed in life before going there - Backgammon. Being, a person who is absolutely fascinated by board games, backgammon was a totally awesome gift to my hobby-list. I played with him with quite often, usually on sundays. He took me to a friend of his, who also loved the game and we had a game in his well-maintained porch with cans of Apple Cidre. Once cannot not like the taste of Apple Cidres.    Cobbled Streets   The most awesome part of the trip was when and a friend of mine, who was also interning in the same university decided to go on a rather spontaneous unplanned trip to a village.(It's just a village, we had no idea which village.) All we wanted was to see the Canadian countryside and the Laurentians were quite close and it would have been a crime to miss them. So yes - a village in the Laurentians. it would have been similarly insane to not atleast spend a night there. Keeping these in mind, we decided to spend a night somewhere in the Laurentians and I searched for a place on Airbnb in the general vicinity of the Laurentians within a reasonable price and we stumbled a pretty house in the woods. We planned to get the morning metro to the Montmorency station and then a local bus to a village called St. Jerome, which also happened to be the starting place of a cycling path called Petit train du Nord(The small train of the North). It was initally a rail track, which they converted to a cycling path when the place went outdated. It had a beautiful terrain through the mountains finally ending in Mont Laurier. We wished to go though the tour, atleast partly, however it was too costly. So, back to our traevelling plans. It all went wrong when we woke up late and missed the first metro, which in turn made us miss the bus to St. Jerome and which in turn made us miss the bus to Val David. Oh! I really I havent told you where we were going. We were going to a small village that lies somewhere in the centre of Les Petit train du Nord called Val David.    Val David   The good thing is, this forced us to spend some time in St. Jerome. There is a pretty little stream cutting through the village, a peculiar church(that somehow looked like a huge bakery to me), a contemporary art museum and a lot of people on cycle. Apparently, everyone was riding a cycle. Abled people rode high cycles, stunt cycles, mountain bikes or racing bikes. Differently abled people rode battery powered wheel chairs and some of them were pretty cool. We had a lunch of hot dog and vanilla Ice cream. it was a task in itself trying to order them as people didn't speak english in the place. It was mostly show-and-tell. We ended up buying tickets for a different bus and the bus was pretty luxurious. We sped through a road though the Laurentians. Laurentians is an old range and has hence eroded over time. It is a popular skiing place , not the right time though. There were patches of ski trails that were clearly visible though the woods. I have always loved woods and mountains more than oceans for the serenity and the colorful scenery it provides. Along with us on the highway, there were a lot of pickup trucks and most of them carried bicycles on their backs. These people were going to start cycling on the train from Mont Laurier(i.e. in the upper part of the Laurentians) and come down to St. Jerome. I will put that on my bucketlist for now. After a very short ride the bus dropped us off at the crossing of Val David. We were quite hungry and we noticed a Tim Hortons. Tim Hortons is the biggest fast food chain(takeaway food chain) in Canada and its quite better than the McDs there. I had some sandwich those guys made along with coffee and home fries. However, the girl at the counter forgot to add the fries and the coffee and charged us for the sandwich only. When I mentioned that I had asked for fries and coffee too, even after insisting repeatedly she didn't take our money and gave them to us for free. Somehow, I hold free food-givers quite high in my list of respected people.    Starting point of Petit Train du Nord   Our house was some 5-6 km away i.e. inside Val David and there wasn't any public transport available. Assisted by google maps and hoping for free rides, we started walking. It was 6 and still bright. Curious as we were, we took a diversion when we saw a lake on the map. We walked via the cycling trail till we reached a lake and it was such a pretty lake. Then we noticed a regional parc near it and just to be clear parcs mean some kind of forests and so, we decided to make it to the parc. On reaching the parc, we asked the guard if we could go through the parc. He told us to come back tomorrow as it would get pretty dark soon and it wouldn't be safe. I remember him telling us there weren't many bear parc as the parc wasn't too big and we wondered , who would want to go hiking in a parc with bears and canadian bears are not the cute kind.    A house by the lake   So, we walked back all the way and started walking towrds the house, which was still 5-6 kms away. It was possibly one of the most beautiful roads, I have ever walked on. There were curious little bungalows on either sides of the road, not too dense but one every once in a while. Some of the houses were pretty cool.    A beautiful house   And on we went down the road for another couple of hours. Went by houses and lakes and a car once in a while too. A very sweet gesture, we noticed, of the people down there was that every one passing by us in a car would not at us. Nevertheless, amidst all these interesting nodding interactions, we reached the house, we were to stay in for the night. Dont go by the looks though. Old though it might look, it was pretty well furnished in the inside. When we reached, the hosts were out but we had the security key and so we let ourselves in, cleaned up and waited inside(a bit hesitant to use things) for the hosts to arrive.The house was backed by the woods and we slept that night staring out a huge window into a very dense wood. It was a bit scary and I am not ashamed to admit it.    House in the woods   Well, the next morning we walked all the way back to the Tim Hortons for breakfast(i.e. 2 and a half hours) and then walked to the parc and then began hiking. The parc had very human touch and it was dense forest with multiple trails crisscrossing through it. We went through one that led to a peak called Mt. Condor. We went to both the Nord(North) and Sud(South) quest of the peak. I had adopted a stick that day to help me deal with the big blood sucking flies/mosquitoes that appeared once in a while and appeared to have imprinted on me. I did read somewhere that this is because I have an attractive body odour. I doubt it though. It never works with any other being other than mosquitoes.    Me with my stick   To put things in a shell, I was highly impressed by the beauty and the serenity of the place and the friendliness of the place and also the fact that I scaled my first official peal(Only 440 metres though)    The trail   The next few weeks went pretty quietly and busy with work. We,(Shivsankar and me), went out almost every night to visit parts of the city. We would often take the bus and get down somewhere and start walking. One night, before we had the bus pass,miser that we were, we didnt want to spend on a bus ticket and we walk almost 16km around Mont Royal to walk to our home. It was 1 am by the team we reached. Other days, we went to a pub called Randolph, which was a board games pub and it was so beautiful and we had grown a friendly acquaintance with the gamemasters, bartender and waiters. Randolph had been mine most frequently visited place in Montreal. I often wondered during my visits to the pub, how wonderful a job it was to teach people how to play board games and then get paid for it.(Canada also has some minimum wage law I guess) We would also take the Metro to other places like Place des Arts and St. Catherine etc. Another person, without whom the trip wouldn't have been as enjoyable as it was is Anirudh. We would often go out to eat(Not many times though). We went to a Jazz festival and I discovered I absolutely love Jazz music though it might have just been the concert effect. It was the grand show by Jamie Cullum and I loved the song When I get Famous. I also had another tasty pizza at an Italian Place, which I had previously visited with Shivsankar. I was totally amazed at the number of different kinds of pizza they had to offer and the number was around 70. Pizzas were quite different there than it is here. The crust is usually thinner and they have sie fillings beneath the cheese which forms a kind of layer and then they have some toppings on it. I dont understand why they want to eat it with fork. A couple of days, I went to a greek place, where I discovered another culinary sweetheart called Souvlaki in Tsaziki. I dont think I can do justice to that great dish by trying to describe it. So just google it and watch the imagies and drool. A few dys before I left, another guy from my lab Tanel Parnamaa finished his intern and went back to Estonia. So, we went out to see him off. We went to a Japanese Raman place and yes, it was awesome too. I believe I had octopus that day too. So, I tasted quite a lot different cuisines there including lebanese, portugese, french, peruvian, greek, thai, vietnamese, quebecoise and Italian. Maybe I will write a seperate post about the food there. It is just that I forget to take pictures before eating them. There was a place called Brulerie's and Luc de Lorraine quite close to the University. One should absolutely visit them. Brulerie is a really cheap place for weekdays early breakfast. It gives you a choice of meat, a couple of eggs(in whatever style), bread, home fries , fruits and unlimited coffee for only 7$, which is cheap. Don't convert it to INR. Just take my word for it that it is cheap. Luc de Lorraine is however pretty costly but serves absolutely delicious french lunch.    Dinner for Tanel with Dima, Anirudh, Krishna, Rosemary and Zander   I had my parting dinner in a vegan thai place with some pretty awesome people including Anirudh, Ishmael, Zander, Rosemary and Krishna.    My Dinner   I can't explain how badly I miss the place. It is an absolutely must-go place for - everyone.    Will come back.   ","date":1469554154,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579777673,"objectID":"7271bb720a79d60a525e53c5e6f8b5ee","permalink":"amartya18x.github.io/post/montreal/","publishdate":"2016-07-26T18:29:14+01:00","relpermalink":"amartya18x.github.io/post/montreal/","section":"post","summary":"Montreal from Mont Royal   As I said before, during the Summers of 2016, I went to Montreal to work as a Research Intern in MILA(Montreal Institute of Learning Algorithms) and there is no way I cannot admit, it was a very wonderful experience. Apart from being a very information-rich span of two and a half months, it was a very beautiful place to visit. In fact, it was too beautiful to not write a separate post about the non-technical experiences of my visit.","tags":["personal"],"title":"Montreal","type":"post"}]